{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (106800, 15)\n",
      "Targets ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127492639</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3887563113</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1142670488</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2718991173</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3080632009</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n",
       "0  1628180742           0                       0.0          353733   \n",
       "1  1628180742           1                       6.0          353733   \n",
       "2  1628180742           2                       8.0          353733   \n",
       "3  1628180742           3                      18.0          353733   \n",
       "4  1628180742           4                      24.0          353733   \n",
       "\n",
       "   spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n",
       "0                   0                               0.0   127492639   \n",
       "1                   1                               6.0  3887563113   \n",
       "2                   2                               8.0  1142670488   \n",
       "3                   3                              18.0  2718991173   \n",
       "4                   4                              24.0  3080632009   \n",
       "\n",
       "   patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  lrda_vote  \\\n",
       "0       42516          Seizure             3         0         0          0   \n",
       "1       42516          Seizure             3         0         0          0   \n",
       "2       42516          Seizure             3         0         0          0   \n",
       "3       42516          Seizure             3         0         0          0   \n",
       "4       42516          Seizure             3         0         0          0   \n",
       "\n",
       "   grda_vote  other_vote  \n",
       "0          0           0  \n",
       "1          0           0  \n",
       "2          0           0  \n",
       "3          0           0  \n",
       "4          0           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "TARGETS = df.columns[-6:]\n",
    "print('Train shape:', df.shape )\n",
    "print('Targets', list(TARGETS))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train non-overlapp eeg_id shape: (17089, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>789577333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>LPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>14960202</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>5955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>618728447</td>\n",
       "      <td>908.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>38549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>52296320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1629671</td>\n",
       "      <td>2036345030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>37481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Seizure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1895581</td>\n",
       "      <td>128369999</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>47999</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2061593</td>\n",
       "      <td>320962633</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>23828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2078097</td>\n",
       "      <td>2074135650</td>\n",
       "      <td>3342.0</td>\n",
       "      <td>3342.0</td>\n",
       "      <td>61174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2366870</td>\n",
       "      <td>1232582129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2482631</td>\n",
       "      <td>978166025</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>20606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2521897</td>\n",
       "      <td>673742515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>62117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2918824</td>\n",
       "      <td>1211648246</td>\n",
       "      <td>3462.0</td>\n",
       "      <td>3462.0</td>\n",
       "      <td>14965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Seizure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3108700</td>\n",
       "      <td>223960986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3625731</td>\n",
       "      <td>2091405434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GRDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3851658</td>\n",
       "      <td>1331405712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LRDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3907459</td>\n",
       "      <td>1343094925</td>\n",
       "      <td>3688.0</td>\n",
       "      <td>3694.0</td>\n",
       "      <td>6489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>LRDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4431217</td>\n",
       "      <td>1459125071</td>\n",
       "      <td>52.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>49713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4454049</td>\n",
       "      <td>1313185981</td>\n",
       "      <td>342.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>44475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>GPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4559645</td>\n",
       "      <td>1902315832</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>22195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GRDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eeg_id  spectrogram_id     min     max  patient_id  seizure_vote  \\\n",
       "0    568657       789577333     0.0    16.0       20654      0.000000   \n",
       "1    582999      1552638400     0.0    38.0       20230      0.000000   \n",
       "2    642382        14960202  1008.0  1032.0        5955      0.000000   \n",
       "3    751790       618728447   908.0   908.0       38549      0.000000   \n",
       "4    778705        52296320     0.0     0.0       40955      0.000000   \n",
       "5   1629671      2036345030     0.0   160.0       37481      1.000000   \n",
       "6   1895581       128369999  1138.0  1138.0       47999      0.076923   \n",
       "7   2061593       320962633  1450.0  1450.0       23828      0.000000   \n",
       "8   2078097      2074135650  3342.0  3342.0       61174      0.000000   \n",
       "9   2366870      1232582129     0.0    30.0       23633      0.000000   \n",
       "10  2482631       978166025  1902.0  1944.0       20606      0.000000   \n",
       "11  2521897       673742515     0.0     4.0       62117      0.000000   \n",
       "12  2918824      1211648246  3462.0  3462.0       14965      1.000000   \n",
       "13  3108700       223960986     0.0     0.0       55677      0.000000   \n",
       "14  3625731      2091405434     0.0     0.0        6935      0.000000   \n",
       "15  3851658      1331405712     0.0     0.0       30631      0.000000   \n",
       "16  3907459      1343094925  3688.0  3694.0        6489      0.000000   \n",
       "17  4431217      1459125071    52.0   124.0       49713      0.000000   \n",
       "18  4454049      1313185981   342.0   380.0       44475      0.000000   \n",
       "19  4559645      1902315832  2436.0  2436.0       22195      0.000000   \n",
       "\n",
       "    lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote   target  \n",
       "0   0.000000  0.250000   0.000000   0.166667    0.583333    Other  \n",
       "1   0.857143  0.000000   0.071429   0.000000    0.071429      LPD  \n",
       "2   0.000000  0.000000   0.000000   0.000000    1.000000    Other  \n",
       "3   0.000000  1.000000   0.000000   0.000000    0.000000      GPD  \n",
       "4   0.000000  0.000000   0.000000   0.000000    1.000000    Other  \n",
       "5   0.000000  0.000000   0.000000   0.000000    0.000000  Seizure  \n",
       "6   0.000000  0.000000   0.000000   0.076923    0.846154    Other  \n",
       "7   0.000000  0.000000   0.000000   0.000000    1.000000    Other  \n",
       "8   0.000000  0.000000   0.000000   0.000000    1.000000    Other  \n",
       "9   0.333333  0.000000   0.000000   0.000000    0.666667    Other  \n",
       "10  0.000000  0.133333   0.066667   0.133333    0.666667    Other  \n",
       "11  0.000000  0.083333   0.083333   0.333333    0.500000    Other  \n",
       "12  0.000000  0.000000   0.000000   0.000000    0.000000  Seizure  \n",
       "13  0.000000  0.000000   0.000000   0.000000    1.000000    Other  \n",
       "14  0.000000  0.000000   0.000000   1.000000    0.000000     GRDA  \n",
       "15  0.000000  0.000000   1.000000   0.000000    0.000000     LRDA  \n",
       "16  0.000000  0.000000   0.666667   0.000000    0.333333     LRDA  \n",
       "17  1.000000  0.000000   0.000000   0.000000    0.000000      LPD  \n",
       "18  0.000000  0.666667   0.000000   0.000000    0.333333      GPD  \n",
       "19  0.000000  0.000000   0.000000   1.000000    0.000000     GRDA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n",
    "    {'spectrogram_id':'first','spectrogram_label_offset_seconds':'min'})\n",
    "train.columns = ['spectrogram_id','min']\n",
    "\n",
    "tmp = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n",
    "    {'spectrogram_label_offset_seconds':'max'})\n",
    "train['max'] = tmp\n",
    "\n",
    "tmp = df.groupby('eeg_id')[['patient_id']].agg('first')\n",
    "train['patient_id'] = tmp\n",
    "\n",
    "tmp = df.groupby('eeg_id')[TARGETS].agg('sum')\n",
    "for t in TARGETS:\n",
    "    train[t] = tmp[t].values\n",
    "    \n",
    "y_data = train[TARGETS].values\n",
    "y_data = y_data / y_data.sum(axis=1,keepdims=True)\n",
    "train[TARGETS] = y_data\n",
    "\n",
    "tmp = df.groupby('eeg_id')[['expert_consensus']].agg('first')\n",
    "train['target'] = tmp\n",
    "\n",
    "train = train.reset_index()\n",
    "print('Train non-overlapp eeg_id shape:', train.shape )\n",
    "\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>LPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1629671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Seizure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1895581</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2061593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2078097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2366870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2482631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2521897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2918824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Seizure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3108700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3625731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GRDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3851658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LRDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3907459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>LRDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4431217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4454049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>GPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4559645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GRDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eeg_id  seizure_vote  lpd_vote  gpd_vote  lrda_vote  grda_vote  \\\n",
       "0    568657      0.000000  0.000000  0.250000   0.000000   0.166667   \n",
       "1    582999      0.000000  0.857143  0.000000   0.071429   0.000000   \n",
       "2    642382      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "3    751790      0.000000  0.000000  1.000000   0.000000   0.000000   \n",
       "4    778705      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "5   1629671      1.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "6   1895581      0.076923  0.000000  0.000000   0.000000   0.076923   \n",
       "7   2061593      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "8   2078097      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "9   2366870      0.000000  0.333333  0.000000   0.000000   0.000000   \n",
       "10  2482631      0.000000  0.000000  0.133333   0.066667   0.133333   \n",
       "11  2521897      0.000000  0.000000  0.083333   0.083333   0.333333   \n",
       "12  2918824      1.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "13  3108700      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "14  3625731      0.000000  0.000000  0.000000   0.000000   1.000000   \n",
       "15  3851658      0.000000  0.000000  0.000000   1.000000   0.000000   \n",
       "16  3907459      0.000000  0.000000  0.000000   0.666667   0.000000   \n",
       "17  4431217      0.000000  1.000000  0.000000   0.000000   0.000000   \n",
       "18  4454049      0.000000  0.000000  0.666667   0.000000   0.000000   \n",
       "19  4559645      0.000000  0.000000  0.000000   0.000000   1.000000   \n",
       "\n",
       "    other_vote   target  \n",
       "0     0.583333    Other  \n",
       "1     0.071429      LPD  \n",
       "2     1.000000    Other  \n",
       "3     0.000000      GPD  \n",
       "4     1.000000    Other  \n",
       "5     0.000000  Seizure  \n",
       "6     0.846154    Other  \n",
       "7     1.000000    Other  \n",
       "8     1.000000    Other  \n",
       "9     0.666667    Other  \n",
       "10    0.666667    Other  \n",
       "11    0.500000    Other  \n",
       "12    0.000000  Seizure  \n",
       "13    1.000000    Other  \n",
       "14    0.000000     GRDA  \n",
       "15    0.000000     LRDA  \n",
       "16    0.333333     LRDA  \n",
       "17    0.000000      LPD  \n",
       "18    0.333333      GPD  \n",
       "19    0.000000     GRDA  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "ycol = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "cd = {'Seizure': 'seizure_vote', 'GPD': 'gpd_vote', 'LRDA': 'lrda_vote', 'Other': 'other_vote', 'GRDA': 'grda_vote', 'LPD': 'lpd_vote'}\n",
    "\n",
    "\n",
    "eeg_id_col = train.iloc[:, 0]  \n",
    "prob_cols = train.iloc[:, -7:-1]  \n",
    "label_col = train.iloc[:, -1] \n",
    "\n",
    "\n",
    "prob_cols = prob_cols.astype(\"float32\")\n",
    "\n",
    "\n",
    "prob_cols_normalized = prob_cols.div(prob_cols.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "normalized_data = pd.concat([eeg_id_col, prob_cols_normalized, label_col], axis=1)\n",
    "\n",
    "normalized_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.to_csv(\"normalized_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = pd.read_csv(\"normalized_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = normalized_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_PATH = 'train_eegs/'\n",
    "train_path = 'normalized_data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy.signal import butter, sosfilt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from EGGDataset import EEGDataset\n",
    "dataset = EEGDataset(train_path, EEG_PATH)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, test_dataset, train_label, test_label = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=8, num_classes=6):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(p=0.75)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(128 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # reshape for LSTM\n",
    "        batch_size, channels, seq_length = x.size()\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # First LSTM layer\n",
    "        x, _ = self.lstm1(x)\n",
    "\n",
    "        # Second LSTM layer\n",
    "        x, _ = self.lstm2(x)\n",
    "        \n",
    "        # Attention layer\n",
    "        att_weights = F.softmax(self.attention(x), dim=1)\n",
    "        x = torch.sum(att_weights * x, dim=1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "    \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of class names\n",
    "class_names = ['Seizure', 'LPD', 'GDP', 'LRDA', 'GRDA', 'Other']\n",
    "\n",
    "# Create a dictionary to map class names to indices\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Create a dictionary to map indices to class names\n",
    "idx_to_class = {idx: class_name for idx, class_name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader: 1/428 - 1715776583.45s\n",
      "Dataloader: 2/428 - 10.53s\n",
      "Dataloader: 3/428 - 9.36s\n",
      "Dataloader: 4/428 - 8.44s\n",
      "Dataloader: 5/428 - 8.78s\n",
      "Dataloader: 6/428 - 9.16s\n",
      "Dataloader: 7/428 - 10.52s\n",
      "Dataloader: 8/428 - 10.61s\n",
      "Dataloader: 9/428 - 8.26s\n",
      "Dataloader: 10/428 - 8.31s\n",
      "Dataloader: 11/428 - 9.18s\n",
      "Dataloader: 12/428 - 8.87s\n",
      "Dataloader: 13/428 - 8.97s\n",
      "Dataloader: 14/428 - 9.8s\n",
      "Dataloader: 15/428 - 10.42s\n",
      "Dataloader: 16/428 - 10.83s\n",
      "Dataloader: 17/428 - 14.03s\n",
      "Dataloader: 18/428 - 13.61s\n",
      "Dataloader: 19/428 - 9.76s\n",
      "Dataloader: 20/428 - 9.48s\n",
      "Dataloader: 21/428 - 9.75s\n",
      "Dataloader: 22/428 - 9.93s\n",
      "Dataloader: 23/428 - 9.85s\n",
      "Dataloader: 24/428 - 10.83s\n",
      "Dataloader: 25/428 - 8.65s\n",
      "Dataloader: 26/428 - 8.82s\n",
      "Dataloader: 27/428 - 9.38s\n",
      "Dataloader: 28/428 - 9.73s\n",
      "Dataloader: 29/428 - 11.17s\n",
      "Dataloader: 30/428 - 10.86s\n",
      "Dataloader: 31/428 - 10.54s\n",
      "Dataloader: 32/428 - 9.51s\n",
      "Dataloader: 33/428 - 10.1s\n",
      "Dataloader: 34/428 - 10.42s\n",
      "Dataloader: 35/428 - 10.28s\n",
      "Dataloader: 36/428 - 9.14s\n",
      "Dataloader: 37/428 - 9.64s\n",
      "Dataloader: 38/428 - 9.49s\n",
      "Dataloader: 39/428 - 9.13s\n",
      "Dataloader: 40/428 - 9.29s\n",
      "Dataloader: 41/428 - 10.12s\n",
      "Dataloader: 42/428 - 8.81s\n",
      "Dataloader: 43/428 - 8.69s\n",
      "Dataloader: 44/428 - 9.49s\n",
      "Dataloader: 45/428 - 9.83s\n",
      "Dataloader: 46/428 - 8.95s\n",
      "Dataloader: 47/428 - 9.29s\n",
      "Dataloader: 48/428 - 9.73s\n",
      "Dataloader: 49/428 - 9.3s\n",
      "Dataloader: 50/428 - 10.42s\n",
      "Dataloader: 51/428 - 8.85s\n",
      "Dataloader: 52/428 - 9.29s\n",
      "Dataloader: 53/428 - 9.55s\n",
      "Dataloader: 54/428 - 9.1s\n",
      "Dataloader: 55/428 - 9.65s\n",
      "Dataloader: 56/428 - 9.34s\n",
      "Dataloader: 57/428 - 9.9s\n",
      "Dataloader: 58/428 - 9.8s\n",
      "Dataloader: 59/428 - 9.32s\n",
      "Dataloader: 60/428 - 9.84s\n",
      "Dataloader: 61/428 - 10.39s\n",
      "Dataloader: 62/428 - 9.92s\n",
      "Dataloader: 63/428 - 9.25s\n",
      "Dataloader: 64/428 - 8.7s\n",
      "Dataloader: 65/428 - 9.37s\n",
      "Dataloader: 66/428 - 9.31s\n",
      "Dataloader: 67/428 - 9.4s\n",
      "Dataloader: 68/428 - 9.25s\n",
      "Dataloader: 69/428 - 9.57s\n",
      "Dataloader: 70/428 - 9.98s\n",
      "Dataloader: 71/428 - 9.36s\n",
      "Dataloader: 72/428 - 9.49s\n",
      "Dataloader: 73/428 - 9.54s\n",
      "Dataloader: 74/428 - 9.91s\n",
      "Dataloader: 75/428 - 9.74s\n",
      "Dataloader: 76/428 - 8.82s\n",
      "Dataloader: 77/428 - 10.51s\n",
      "Dataloader: 78/428 - 8.72s\n",
      "Dataloader: 79/428 - 9.35s\n",
      "Dataloader: 80/428 - 8.7s\n",
      "Dataloader: 81/428 - 9.01s\n",
      "Dataloader: 82/428 - 8.43s\n",
      "Dataloader: 83/428 - 9.16s\n",
      "Dataloader: 84/428 - 9.36s\n",
      "Dataloader: 85/428 - 9.42s\n",
      "Dataloader: 86/428 - 9.35s\n",
      "Dataloader: 87/428 - 9.36s\n",
      "Dataloader: 88/428 - 9.79s\n",
      "Dataloader: 89/428 - 9.35s\n",
      "Dataloader: 90/428 - 13.15s\n",
      "Dataloader: 91/428 - 14.96s\n",
      "Dataloader: 92/428 - 13.06s\n",
      "Dataloader: 93/428 - 11.86s\n",
      "Dataloader: 94/428 - 12.46s\n",
      "Dataloader: 95/428 - 12.78s\n",
      "Dataloader: 96/428 - 18.48s\n",
      "Dataloader: 97/428 - 11.15s\n",
      "Dataloader: 98/428 - 10.81s\n",
      "Dataloader: 99/428 - 10.49s\n",
      "Dataloader: 100/428 - 11.09s\n",
      "Dataloader: 101/428 - 10.2s\n",
      "Dataloader: 102/428 - 10.14s\n",
      "Dataloader: 103/428 - 10.35s\n",
      "Dataloader: 104/428 - 12.86s\n",
      "Dataloader: 105/428 - 11.17s\n",
      "Dataloader: 106/428 - 18.8s\n",
      "Dataloader: 107/428 - 16.59s\n",
      "Dataloader: 108/428 - 14.38s\n",
      "Dataloader: 109/428 - 13.11s\n",
      "Dataloader: 110/428 - 13.69s\n",
      "Dataloader: 111/428 - 14.21s\n",
      "Dataloader: 112/428 - 13.42s\n",
      "Dataloader: 113/428 - 10.11s\n",
      "Dataloader: 114/428 - 10.48s\n",
      "Dataloader: 115/428 - 10.83s\n",
      "Dataloader: 116/428 - 11.45s\n",
      "Dataloader: 117/428 - 10.14s\n",
      "Dataloader: 118/428 - 9.87s\n",
      "Dataloader: 119/428 - 9.15s\n",
      "Dataloader: 120/428 - 9.42s\n",
      "Dataloader: 121/428 - 10.19s\n",
      "Dataloader: 122/428 - 9.59s\n",
      "Dataloader: 123/428 - 10.13s\n",
      "Dataloader: 124/428 - 9.74s\n",
      "Dataloader: 125/428 - 9.64s\n",
      "Dataloader: 126/428 - 9.77s\n",
      "Dataloader: 127/428 - 9.07s\n",
      "Dataloader: 128/428 - 10.28s\n",
      "Dataloader: 129/428 - 9.51s\n",
      "Dataloader: 130/428 - 9.45s\n",
      "Dataloader: 131/428 - 10.25s\n",
      "Dataloader: 132/428 - 9.54s\n",
      "Dataloader: 133/428 - 9.35s\n",
      "Dataloader: 134/428 - 9.54s\n",
      "Dataloader: 135/428 - 10.07s\n",
      "Dataloader: 136/428 - 9.47s\n",
      "Dataloader: 137/428 - 9.77s\n",
      "Dataloader: 138/428 - 9.81s\n",
      "Dataloader: 139/428 - 8.74s\n",
      "Dataloader: 140/428 - 9.62s\n",
      "Dataloader: 141/428 - 9.46s\n",
      "Dataloader: 142/428 - 9.3s\n",
      "Dataloader: 143/428 - 9.26s\n",
      "Dataloader: 144/428 - 9.0s\n",
      "Dataloader: 145/428 - 9.07s\n",
      "Dataloader: 146/428 - 9.73s\n",
      "Dataloader: 147/428 - 9.94s\n",
      "Dataloader: 148/428 - 10.75s\n",
      "Dataloader: 149/428 - 10.35s\n",
      "Dataloader: 150/428 - 10.34s\n",
      "Dataloader: 151/428 - 11.52s\n",
      "Dataloader: 152/428 - 9.4s\n",
      "Dataloader: 153/428 - 9.03s\n",
      "Dataloader: 154/428 - 9.36s\n",
      "Dataloader: 155/428 - 10.26s\n",
      "Dataloader: 156/428 - 9.41s\n",
      "Dataloader: 157/428 - 8.62s\n",
      "Dataloader: 158/428 - 9.49s\n",
      "Dataloader: 159/428 - 9.02s\n",
      "Dataloader: 160/428 - 9.59s\n",
      "Dataloader: 161/428 - 10.37s\n",
      "Dataloader: 162/428 - 9.02s\n",
      "Dataloader: 163/428 - 9.54s\n",
      "Dataloader: 164/428 - 10.53s\n",
      "Dataloader: 165/428 - 10.79s\n",
      "Dataloader: 166/428 - 10.34s\n",
      "Dataloader: 167/428 - 9.14s\n",
      "Dataloader: 168/428 - 9.69s\n",
      "Dataloader: 169/428 - 9.82s\n",
      "Dataloader: 170/428 - 9.01s\n",
      "Dataloader: 171/428 - 8.66s\n",
      "Dataloader: 172/428 - 9.04s\n",
      "Dataloader: 173/428 - 9.11s\n",
      "Dataloader: 174/428 - 9.18s\n",
      "Dataloader: 175/428 - 8.7s\n",
      "Dataloader: 176/428 - 9.08s\n",
      "Dataloader: 177/428 - 10.19s\n",
      "Dataloader: 178/428 - 8.41s\n",
      "Dataloader: 179/428 - 11.18s\n",
      "Dataloader: 180/428 - 9.03s\n",
      "Dataloader: 181/428 - 9.61s\n",
      "Dataloader: 182/428 - 9.31s\n",
      "Dataloader: 183/428 - 9.56s\n",
      "Dataloader: 184/428 - 8.5s\n",
      "Dataloader: 185/428 - 8.51s\n",
      "Dataloader: 186/428 - 9.27s\n",
      "Dataloader: 187/428 - 9.54s\n",
      "Dataloader: 188/428 - 9.12s\n",
      "Dataloader: 189/428 - 8.87s\n",
      "Dataloader: 190/428 - 10.71s\n",
      "Dataloader: 191/428 - 10.19s\n",
      "Dataloader: 192/428 - 9.53s\n",
      "Dataloader: 193/428 - 9.15s\n",
      "Dataloader: 194/428 - 9.17s\n",
      "Dataloader: 195/428 - 10.04s\n",
      "Dataloader: 196/428 - 10.02s\n",
      "Dataloader: 197/428 - 9.52s\n",
      "Dataloader: 198/428 - 9.21s\n",
      "Dataloader: 199/428 - 8.98s\n",
      "Dataloader: 200/428 - 8.63s\n",
      "Dataloader: 201/428 - 9.08s\n",
      "Dataloader: 202/428 - 9.17s\n",
      "Dataloader: 203/428 - 9.59s\n",
      "Dataloader: 204/428 - 8.99s\n",
      "Dataloader: 205/428 - 9.11s\n",
      "Dataloader: 206/428 - 9.14s\n",
      "Dataloader: 207/428 - 8.58s\n",
      "Dataloader: 208/428 - 8.89s\n",
      "Dataloader: 209/428 - 9.53s\n",
      "Dataloader: 210/428 - 8.71s\n",
      "Dataloader: 211/428 - 10.62s\n",
      "Dataloader: 212/428 - 10.34s\n",
      "Dataloader: 213/428 - 9.89s\n",
      "Dataloader: 214/428 - 9.14s\n",
      "Dataloader: 215/428 - 9.25s\n",
      "Dataloader: 216/428 - 8.93s\n",
      "Dataloader: 217/428 - 9.11s\n",
      "Dataloader: 218/428 - 9.95s\n",
      "Dataloader: 219/428 - 8.49s\n",
      "Dataloader: 220/428 - 10.64s\n",
      "Dataloader: 221/428 - 8.75s\n",
      "Dataloader: 222/428 - 8.89s\n",
      "Dataloader: 223/428 - 9.39s\n",
      "Dataloader: 224/428 - 9.54s\n",
      "Dataloader: 225/428 - 9.21s\n",
      "Dataloader: 226/428 - 8.89s\n",
      "Dataloader: 227/428 - 8.85s\n",
      "Dataloader: 228/428 - 9.09s\n",
      "Dataloader: 229/428 - 9.4s\n",
      "Dataloader: 230/428 - 8.74s\n",
      "Dataloader: 231/428 - 9.91s\n",
      "Dataloader: 232/428 - 9.33s\n",
      "Dataloader: 233/428 - 9.15s\n",
      "Dataloader: 234/428 - 9.24s\n",
      "Dataloader: 235/428 - 8.93s\n",
      "Dataloader: 236/428 - 9.3s\n",
      "Dataloader: 237/428 - 10.02s\n",
      "Dataloader: 238/428 - 9.39s\n",
      "Dataloader: 239/428 - 11.22s\n",
      "Dataloader: 240/428 - 8.59s\n",
      "Dataloader: 241/428 - 9.5s\n",
      "Dataloader: 242/428 - 8.75s\n",
      "Dataloader: 243/428 - 9.3s\n",
      "Dataloader: 244/428 - 9.65s\n",
      "Dataloader: 245/428 - 8.64s\n",
      "Dataloader: 246/428 - 8.81s\n",
      "Dataloader: 247/428 - 8.53s\n",
      "Dataloader: 248/428 - 8.7s\n",
      "Dataloader: 249/428 - 8.74s\n",
      "Dataloader: 250/428 - 9.36s\n",
      "Dataloader: 251/428 - 8.86s\n",
      "Dataloader: 252/428 - 8.54s\n",
      "Dataloader: 253/428 - 9.35s\n",
      "Dataloader: 254/428 - 8.91s\n",
      "Dataloader: 255/428 - 10.06s\n",
      "Dataloader: 256/428 - 9.56s\n",
      "Dataloader: 257/428 - 10.51s\n",
      "Dataloader: 258/428 - 8.5s\n",
      "Dataloader: 259/428 - 9.58s\n",
      "Dataloader: 260/428 - 8.9s\n",
      "Dataloader: 261/428 - 9.36s\n",
      "Dataloader: 262/428 - 8.66s\n",
      "Dataloader: 263/428 - 8.92s\n",
      "Dataloader: 264/428 - 8.96s\n",
      "Dataloader: 265/428 - 8.18s\n",
      "Dataloader: 266/428 - 9.32s\n",
      "Dataloader: 267/428 - 8.48s\n",
      "Dataloader: 268/428 - 9.82s\n",
      "Dataloader: 269/428 - 9.14s\n",
      "Dataloader: 270/428 - 9.85s\n",
      "Dataloader: 271/428 - 8.73s\n",
      "Dataloader: 272/428 - 10.65s\n",
      "Dataloader: 273/428 - 8.42s\n",
      "Dataloader: 274/428 - 9.33s\n",
      "Dataloader: 275/428 - 9.07s\n",
      "Dataloader: 276/428 - 9.44s\n",
      "Dataloader: 277/428 - 11.06s\n",
      "Dataloader: 278/428 - 9.89s\n",
      "Dataloader: 279/428 - 9.57s\n",
      "Dataloader: 280/428 - 8.76s\n",
      "Dataloader: 281/428 - 8.77s\n",
      "Dataloader: 282/428 - 8.94s\n",
      "Dataloader: 283/428 - 8.77s\n",
      "Dataloader: 284/428 - 9.06s\n",
      "Dataloader: 285/428 - 9.32s\n",
      "Dataloader: 286/428 - 9.67s\n",
      "Dataloader: 287/428 - 9.1s\n",
      "Dataloader: 288/428 - 8.9s\n",
      "Dataloader: 289/428 - 9.23s\n",
      "Dataloader: 290/428 - 9.39s\n",
      "Dataloader: 291/428 - 8.45s\n",
      "Dataloader: 292/428 - 9.74s\n",
      "Dataloader: 293/428 - 9.59s\n",
      "Dataloader: 294/428 - 9.02s\n",
      "Dataloader: 295/428 - 8.55s\n",
      "Dataloader: 296/428 - 9.31s\n",
      "Dataloader: 297/428 - 8.5s\n",
      "Dataloader: 298/428 - 8.94s\n",
      "Dataloader: 299/428 - 9.92s\n",
      "Dataloader: 300/428 - 8.84s\n",
      "Dataloader: 301/428 - 8.54s\n",
      "Dataloader: 302/428 - 8.55s\n",
      "Dataloader: 303/428 - 10.0s\n",
      "Dataloader: 304/428 - 8.78s\n",
      "Dataloader: 305/428 - 8.8s\n",
      "Dataloader: 306/428 - 8.56s\n",
      "Dataloader: 307/428 - 9.09s\n",
      "Dataloader: 308/428 - 8.6s\n",
      "Dataloader: 309/428 - 9.4s\n",
      "Dataloader: 310/428 - 10.18s\n",
      "Dataloader: 311/428 - 10.36s\n",
      "Dataloader: 312/428 - 9.73s\n",
      "Dataloader: 313/428 - 9.26s\n",
      "Dataloader: 314/428 - 9.33s\n",
      "Dataloader: 315/428 - 9.79s\n",
      "Dataloader: 316/428 - 8.8s\n",
      "Dataloader: 317/428 - 8.32s\n",
      "Dataloader: 318/428 - 8.36s\n",
      "Dataloader: 319/428 - 8.33s\n",
      "Dataloader: 320/428 - 8.62s\n",
      "Dataloader: 321/428 - 8.95s\n",
      "Dataloader: 322/428 - 9.33s\n",
      "Dataloader: 323/428 - 9.13s\n",
      "Dataloader: 324/428 - 8.83s\n",
      "Dataloader: 325/428 - 8.99s\n",
      "Dataloader: 326/428 - 9.76s\n",
      "Dataloader: 327/428 - 9.27s\n",
      "Dataloader: 328/428 - 8.46s\n",
      "Dataloader: 329/428 - 8.81s\n",
      "Dataloader: 330/428 - 9.76s\n",
      "Dataloader: 331/428 - 9.4s\n",
      "Dataloader: 332/428 - 9.49s\n",
      "Dataloader: 333/428 - 9.11s\n",
      "Dataloader: 334/428 - 8.82s\n",
      "Dataloader: 335/428 - 9.25s\n",
      "Dataloader: 336/428 - 8.87s\n",
      "Dataloader: 337/428 - 8.75s\n",
      "Dataloader: 338/428 - 8.87s\n",
      "Dataloader: 339/428 - 10.79s\n",
      "Dataloader: 340/428 - 9.59s\n",
      "Dataloader: 341/428 - 8.94s\n",
      "Dataloader: 342/428 - 9.35s\n",
      "Dataloader: 343/428 - 9.74s\n",
      "Dataloader: 344/428 - 10.25s\n",
      "Dataloader: 345/428 - 9.68s\n",
      "Dataloader: 346/428 - 8.63s\n",
      "Dataloader: 347/428 - 8.8s\n",
      "Dataloader: 348/428 - 9.24s\n",
      "Dataloader: 349/428 - 9.63s\n",
      "Dataloader: 350/428 - 8.54s\n",
      "Dataloader: 351/428 - 8.81s\n",
      "Dataloader: 352/428 - 8.64s\n",
      "Dataloader: 353/428 - 9.07s\n",
      "Dataloader: 354/428 - 9.02s\n",
      "Dataloader: 355/428 - 8.65s\n",
      "Dataloader: 356/428 - 9.78s\n",
      "Dataloader: 357/428 - 9.26s\n",
      "Dataloader: 358/428 - 9.38s\n",
      "Dataloader: 359/428 - 8.88s\n",
      "Dataloader: 360/428 - 8.2s\n",
      "Dataloader: 361/428 - 9.7s\n",
      "Dataloader: 362/428 - 8.76s\n",
      "Dataloader: 363/428 - 8.68s\n",
      "Dataloader: 364/428 - 8.7s\n",
      "Dataloader: 365/428 - 8.48s\n",
      "Dataloader: 366/428 - 8.63s\n",
      "Dataloader: 367/428 - 8.69s\n",
      "Dataloader: 368/428 - 9.47s\n",
      "Dataloader: 369/428 - 9.51s\n",
      "Dataloader: 370/428 - 9.74s\n",
      "Dataloader: 371/428 - 8.79s\n",
      "Dataloader: 372/428 - 10.52s\n",
      "Dataloader: 373/428 - 8.83s\n",
      "Dataloader: 374/428 - 8.78s\n",
      "Dataloader: 375/428 - 9.1s\n",
      "Dataloader: 376/428 - 8.67s\n",
      "Dataloader: 377/428 - 9.68s\n",
      "Dataloader: 378/428 - 8.54s\n",
      "Dataloader: 379/428 - 9.38s\n",
      "Dataloader: 380/428 - 8.16s\n",
      "Dataloader: 381/428 - 8.95s\n",
      "Dataloader: 382/428 - 9.1s\n",
      "Dataloader: 383/428 - 9.21s\n",
      "Dataloader: 384/428 - 8.54s\n",
      "Dataloader: 385/428 - 8.57s\n",
      "Dataloader: 386/428 - 9.66s\n",
      "Dataloader: 387/428 - 8.81s\n",
      "Dataloader: 388/428 - 8.96s\n",
      "Dataloader: 389/428 - 8.48s\n",
      "Dataloader: 390/428 - 8.58s\n",
      "Dataloader: 391/428 - 8.83s\n",
      "Dataloader: 392/428 - 9.28s\n",
      "Dataloader: 393/428 - 9.32s\n",
      "Dataloader: 394/428 - 8.41s\n",
      "Dataloader: 395/428 - 9.08s\n",
      "Dataloader: 396/428 - 8.43s\n",
      "Dataloader: 397/428 - 9.06s\n",
      "Dataloader: 398/428 - 8.91s\n",
      "Dataloader: 399/428 - 9.73s\n",
      "Dataloader: 400/428 - 8.72s\n",
      "Dataloader: 401/428 - 8.93s\n",
      "Dataloader: 402/428 - 8.6s\n",
      "Dataloader: 403/428 - 10.02s\n",
      "Dataloader: 404/428 - 9.66s\n",
      "Dataloader: 405/428 - 9.01s\n",
      "Dataloader: 406/428 - 8.66s\n",
      "Dataloader: 407/428 - 8.53s\n",
      "Dataloader: 408/428 - 8.87s\n",
      "Dataloader: 409/428 - 8.55s\n",
      "Dataloader: 410/428 - 10.61s\n",
      "Dataloader: 411/428 - 8.59s\n",
      "Dataloader: 412/428 - 8.52s\n",
      "Dataloader: 413/428 - 8.7s\n",
      "Dataloader: 414/428 - 9.21s\n",
      "Dataloader: 415/428 - 8.89s\n",
      "Dataloader: 416/428 - 8.68s\n",
      "Dataloader: 417/428 - 8.39s\n",
      "Dataloader: 418/428 - 8.68s\n",
      "Dataloader: 419/428 - 9.18s\n",
      "Dataloader: 420/428 - 8.83s\n",
      "Dataloader: 421/428 - 8.37s\n",
      "Dataloader: 422/428 - 11.05s\n",
      "Dataloader: 423/428 - 8.59s\n",
      "Dataloader: 424/428 - 8.74s\n",
      "Dataloader: 425/428 - 8.43s\n",
      "Dataloader: 426/428 - 8.79s\n",
      "Dataloader: 427/428 - 8.56s\n",
      "Dataloader: 428/428 - 9.31s\n",
      "Epoch 1, Loss: 1.5699115052958514\n",
      "Dataloader: 1/428 - 3.2s\n",
      "Dataloader: 2/428 - 8.9s\n",
      "Dataloader: 3/428 - 8.47s\n",
      "Dataloader: 4/428 - 8.75s\n",
      "Dataloader: 5/428 - 8.9s\n",
      "Dataloader: 6/428 - 9.17s\n",
      "Dataloader: 7/428 - 9.5s\n",
      "Dataloader: 8/428 - 9.72s\n",
      "Dataloader: 9/428 - 8.66s\n",
      "Dataloader: 10/428 - 9.36s\n",
      "Dataloader: 11/428 - 9.14s\n",
      "Dataloader: 12/428 - 9.7s\n",
      "Dataloader: 13/428 - 8.41s\n",
      "Dataloader: 14/428 - 9.44s\n",
      "Dataloader: 15/428 - 8.28s\n",
      "Dataloader: 16/428 - 8.56s\n",
      "Dataloader: 17/428 - 9.52s\n",
      "Dataloader: 18/428 - 8.8s\n",
      "Dataloader: 19/428 - 8.86s\n",
      "Dataloader: 20/428 - 9.0s\n",
      "Dataloader: 21/428 - 9.37s\n",
      "Dataloader: 22/428 - 8.81s\n",
      "Dataloader: 23/428 - 8.74s\n",
      "Dataloader: 24/428 - 8.43s\n",
      "Dataloader: 25/428 - 9.17s\n",
      "Dataloader: 26/428 - 9.13s\n",
      "Dataloader: 27/428 - 8.99s\n",
      "Dataloader: 28/428 - 8.25s\n",
      "Dataloader: 29/428 - 9.36s\n",
      "Dataloader: 30/428 - 8.96s\n",
      "Dataloader: 31/428 - 10.03s\n",
      "Dataloader: 32/428 - 9.11s\n",
      "Dataloader: 33/428 - 9.36s\n",
      "Dataloader: 34/428 - 9.39s\n",
      "Dataloader: 35/428 - 8.88s\n",
      "Dataloader: 36/428 - 8.64s\n",
      "Dataloader: 37/428 - 8.93s\n",
      "Dataloader: 38/428 - 9.0s\n",
      "Dataloader: 39/428 - 8.87s\n",
      "Dataloader: 40/428 - 8.72s\n",
      "Dataloader: 41/428 - 9.18s\n",
      "Dataloader: 42/428 - 8.97s\n",
      "Dataloader: 43/428 - 8.98s\n",
      "Dataloader: 44/428 - 9.28s\n",
      "Dataloader: 45/428 - 9.35s\n",
      "Dataloader: 46/428 - 8.73s\n",
      "Dataloader: 47/428 - 9.03s\n",
      "Dataloader: 48/428 - 9.13s\n",
      "Dataloader: 49/428 - 8.83s\n",
      "Dataloader: 50/428 - 9.71s\n",
      "Dataloader: 51/428 - 8.44s\n",
      "Dataloader: 52/428 - 8.95s\n",
      "Dataloader: 53/428 - 9.27s\n",
      "Dataloader: 54/428 - 9.11s\n",
      "Dataloader: 55/428 - 8.43s\n",
      "Dataloader: 56/428 - 8.85s\n",
      "Dataloader: 57/428 - 9.49s\n",
      "Dataloader: 58/428 - 9.13s\n",
      "Dataloader: 59/428 - 8.67s\n",
      "Dataloader: 60/428 - 8.51s\n",
      "Dataloader: 61/428 - 9.44s\n",
      "Dataloader: 62/428 - 9.11s\n",
      "Dataloader: 63/428 - 9.11s\n",
      "Dataloader: 64/428 - 9.11s\n",
      "Dataloader: 65/428 - 8.76s\n",
      "Dataloader: 66/428 - 8.19s\n",
      "Dataloader: 67/428 - 12.58s\n",
      "Dataloader: 68/428 - 14.94s\n",
      "Dataloader: 69/428 - 12.74s\n",
      "Dataloader: 70/428 - 10.58s\n",
      "Dataloader: 71/428 - 11.31s\n",
      "Dataloader: 72/428 - 9.07s\n",
      "Dataloader: 73/428 - 8.98s\n",
      "Dataloader: 74/428 - 8.96s\n",
      "Dataloader: 75/428 - 10.65s\n",
      "Dataloader: 76/428 - 9.28s\n",
      "Dataloader: 77/428 - 9.4s\n",
      "Dataloader: 78/428 - 8.92s\n",
      "Dataloader: 79/428 - 9.35s\n",
      "Dataloader: 80/428 - 8.88s\n",
      "Dataloader: 81/428 - 8.84s\n",
      "Dataloader: 82/428 - 9.31s\n",
      "Dataloader: 83/428 - 9.09s\n",
      "Dataloader: 84/428 - 8.48s\n",
      "Dataloader: 85/428 - 8.95s\n",
      "Dataloader: 86/428 - 9.02s\n",
      "Dataloader: 87/428 - 8.83s\n",
      "Dataloader: 88/428 - 9.74s\n",
      "Dataloader: 89/428 - 9.08s\n",
      "Dataloader: 90/428 - 9.98s\n",
      "Dataloader: 91/428 - 8.65s\n",
      "Dataloader: 92/428 - 9.35s\n",
      "Dataloader: 93/428 - 8.65s\n",
      "Dataloader: 94/428 - 8.34s\n",
      "Dataloader: 95/428 - 9.32s\n",
      "Dataloader: 96/428 - 8.92s\n",
      "Dataloader: 97/428 - 8.98s\n",
      "Dataloader: 98/428 - 9.27s\n",
      "Dataloader: 99/428 - 9.23s\n",
      "Dataloader: 100/428 - 9.07s\n",
      "Dataloader: 101/428 - 8.78s\n",
      "Dataloader: 102/428 - 10.65s\n",
      "Dataloader: 103/428 - 9.1s\n",
      "Dataloader: 104/428 - 20.27s\n",
      "Dataloader: 105/428 - 15.97s\n",
      "Dataloader: 106/428 - 14.12s\n",
      "Dataloader: 107/428 - 14.27s\n",
      "Dataloader: 108/428 - 13.32s\n",
      "Dataloader: 109/428 - 12.49s\n",
      "Dataloader: 110/428 - 8.36s\n",
      "Dataloader: 111/428 - 10.32s\n",
      "Dataloader: 112/428 - 9.76s\n",
      "Dataloader: 113/428 - 8.9s\n",
      "Dataloader: 114/428 - 8.87s\n",
      "Dataloader: 115/428 - 8.91s\n",
      "Dataloader: 116/428 - 9.14s\n",
      "Dataloader: 117/428 - 9.19s\n",
      "Dataloader: 118/428 - 8.93s\n",
      "Dataloader: 119/428 - 8.88s\n",
      "Dataloader: 120/428 - 8.34s\n",
      "Dataloader: 121/428 - 8.86s\n",
      "Dataloader: 122/428 - 9.43s\n",
      "Dataloader: 123/428 - 8.4s\n",
      "Dataloader: 124/428 - 8.92s\n",
      "Dataloader: 125/428 - 8.94s\n",
      "Dataloader: 126/428 - 9.05s\n",
      "Dataloader: 127/428 - 9.52s\n",
      "Dataloader: 128/428 - 9.8s\n",
      "Dataloader: 129/428 - 11.69s\n",
      "Dataloader: 130/428 - 8.64s\n",
      "Dataloader: 131/428 - 9.83s\n",
      "Dataloader: 132/428 - 8.98s\n",
      "Dataloader: 133/428 - 9.56s\n",
      "Dataloader: 134/428 - 9.69s\n",
      "Dataloader: 135/428 - 9.36s\n",
      "Dataloader: 136/428 - 8.7s\n",
      "Dataloader: 137/428 - 9.32s\n",
      "Dataloader: 138/428 - 9.64s\n",
      "Dataloader: 139/428 - 10.45s\n",
      "Dataloader: 140/428 - 8.68s\n",
      "Dataloader: 141/428 - 8.3s\n",
      "Dataloader: 142/428 - 8.07s\n",
      "Dataloader: 143/428 - 9.02s\n",
      "Dataloader: 144/428 - 9.54s\n",
      "Dataloader: 145/428 - 8.39s\n",
      "Dataloader: 146/428 - 8.93s\n",
      "Dataloader: 147/428 - 8.65s\n",
      "Dataloader: 148/428 - 9.37s\n",
      "Dataloader: 149/428 - 8.97s\n",
      "Dataloader: 150/428 - 9.17s\n",
      "Dataloader: 151/428 - 8.82s\n",
      "Dataloader: 152/428 - 8.82s\n",
      "Dataloader: 153/428 - 8.61s\n",
      "Dataloader: 154/428 - 8.95s\n",
      "Dataloader: 155/428 - 8.9s\n",
      "Dataloader: 156/428 - 9.22s\n",
      "Dataloader: 157/428 - 10.08s\n",
      "Dataloader: 158/428 - 9.18s\n",
      "Dataloader: 159/428 - 8.51s\n",
      "Dataloader: 160/428 - 8.54s\n",
      "Dataloader: 161/428 - 8.57s\n",
      "Dataloader: 162/428 - 8.27s\n",
      "Dataloader: 163/428 - 9.43s\n",
      "Dataloader: 164/428 - 8.56s\n",
      "Dataloader: 165/428 - 9.13s\n",
      "Dataloader: 166/428 - 9.69s\n",
      "Dataloader: 167/428 - 9.09s\n",
      "Dataloader: 168/428 - 8.64s\n",
      "Dataloader: 169/428 - 8.77s\n",
      "Dataloader: 170/428 - 9.58s\n",
      "Dataloader: 171/428 - 9.79s\n",
      "Dataloader: 172/428 - 9.04s\n",
      "Dataloader: 173/428 - 9.55s\n",
      "Dataloader: 174/428 - 8.95s\n",
      "Dataloader: 175/428 - 9.21s\n",
      "Dataloader: 176/428 - 8.39s\n",
      "Dataloader: 177/428 - 8.73s\n",
      "Dataloader: 178/428 - 10.22s\n",
      "Dataloader: 179/428 - 9.09s\n",
      "Dataloader: 180/428 - 9.63s\n",
      "Dataloader: 181/428 - 8.39s\n",
      "Dataloader: 182/428 - 8.14s\n",
      "Dataloader: 183/428 - 9.12s\n",
      "Dataloader: 184/428 - 8.66s\n",
      "Dataloader: 185/428 - 9.12s\n",
      "Dataloader: 186/428 - 8.9s\n",
      "Dataloader: 187/428 - 9.16s\n",
      "Dataloader: 188/428 - 8.43s\n",
      "Dataloader: 189/428 - 8.47s\n",
      "Dataloader: 190/428 - 8.91s\n",
      "Dataloader: 191/428 - 9.16s\n",
      "Dataloader: 192/428 - 9.08s\n",
      "Dataloader: 193/428 - 8.95s\n",
      "Dataloader: 194/428 - 9.74s\n",
      "Dataloader: 195/428 - 9.29s\n",
      "Dataloader: 196/428 - 9.3s\n",
      "Dataloader: 197/428 - 9.56s\n",
      "Dataloader: 198/428 - 9.01s\n",
      "Dataloader: 199/428 - 9.02s\n",
      "Dataloader: 200/428 - 10.47s\n",
      "Dataloader: 201/428 - 8.36s\n",
      "Dataloader: 202/428 - 8.32s\n",
      "Dataloader: 203/428 - 8.45s\n",
      "Dataloader: 204/428 - 8.86s\n",
      "Dataloader: 205/428 - 9.6s\n",
      "Dataloader: 206/428 - 8.66s\n",
      "Dataloader: 207/428 - 8.92s\n",
      "Dataloader: 208/428 - 9.11s\n",
      "Dataloader: 209/428 - 8.47s\n",
      "Dataloader: 210/428 - 8.11s\n",
      "Dataloader: 211/428 - 10.14s\n",
      "Dataloader: 212/428 - 10.27s\n",
      "Dataloader: 213/428 - 9.63s\n",
      "Dataloader: 214/428 - 8.09s\n",
      "Dataloader: 215/428 - 9.18s\n",
      "Dataloader: 216/428 - 8.17s\n",
      "Dataloader: 217/428 - 8.64s\n",
      "Dataloader: 218/428 - 8.93s\n",
      "Dataloader: 219/428 - 8.81s\n",
      "Dataloader: 220/428 - 9.26s\n",
      "Dataloader: 221/428 - 9.16s\n",
      "Dataloader: 222/428 - 8.37s\n",
      "Dataloader: 223/428 - 8.88s\n",
      "Dataloader: 224/428 - 9.15s\n",
      "Dataloader: 225/428 - 9.29s\n",
      "Dataloader: 226/428 - 8.33s\n",
      "Dataloader: 227/428 - 9.34s\n",
      "Dataloader: 228/428 - 10.82s\n",
      "Dataloader: 229/428 - 8.67s\n",
      "Dataloader: 230/428 - 8.57s\n",
      "Dataloader: 231/428 - 8.91s\n",
      "Dataloader: 232/428 - 8.37s\n",
      "Dataloader: 233/428 - 9.0s\n",
      "Dataloader: 234/428 - 9.02s\n",
      "Dataloader: 235/428 - 9.06s\n",
      "Dataloader: 236/428 - 8.48s\n",
      "Dataloader: 237/428 - 8.11s\n",
      "Dataloader: 238/428 - 9.29s\n",
      "Dataloader: 239/428 - 9.32s\n",
      "Dataloader: 240/428 - 9.43s\n",
      "Dataloader: 241/428 - 8.94s\n",
      "Dataloader: 242/428 - 8.74s\n",
      "Dataloader: 243/428 - 9.31s\n",
      "Dataloader: 244/428 - 8.66s\n",
      "Dataloader: 245/428 - 9.96s\n",
      "Dataloader: 246/428 - 9.94s\n",
      "Dataloader: 247/428 - 8.9s\n",
      "Dataloader: 248/428 - 9.1s\n",
      "Dataloader: 249/428 - 8.99s\n",
      "Dataloader: 250/428 - 8.63s\n",
      "Dataloader: 251/428 - 8.8s\n",
      "Dataloader: 252/428 - 9.22s\n",
      "Dataloader: 253/428 - 9.04s\n",
      "Dataloader: 254/428 - 9.09s\n",
      "Dataloader: 255/428 - 9.1s\n",
      "Dataloader: 256/428 - 8.8s\n",
      "Dataloader: 257/428 - 8.67s\n",
      "Dataloader: 258/428 - 9.69s\n",
      "Dataloader: 259/428 - 8.75s\n",
      "Dataloader: 260/428 - 9.42s\n",
      "Dataloader: 261/428 - 8.85s\n",
      "Dataloader: 262/428 - 9.03s\n",
      "Dataloader: 263/428 - 9.42s\n",
      "Dataloader: 264/428 - 9.72s\n",
      "Dataloader: 265/428 - 10.14s\n",
      "Dataloader: 266/428 - 8.68s\n",
      "Dataloader: 267/428 - 9.06s\n",
      "Dataloader: 268/428 - 8.92s\n",
      "Dataloader: 269/428 - 8.21s\n",
      "Dataloader: 270/428 - 8.28s\n",
      "Dataloader: 271/428 - 9.09s\n",
      "Dataloader: 272/428 - 9.95s\n",
      "Dataloader: 273/428 - 9.6s\n",
      "Dataloader: 274/428 - 8.89s\n",
      "Dataloader: 275/428 - 9.37s\n",
      "Dataloader: 276/428 - 8.99s\n",
      "Dataloader: 277/428 - 8.66s\n",
      "Dataloader: 278/428 - 9.17s\n",
      "Dataloader: 279/428 - 9.55s\n",
      "Dataloader: 280/428 - 8.61s\n",
      "Dataloader: 281/428 - 8.81s\n",
      "Dataloader: 282/428 - 9.22s\n",
      "Dataloader: 283/428 - 8.39s\n",
      "Dataloader: 284/428 - 8.78s\n",
      "Dataloader: 285/428 - 8.57s\n",
      "Dataloader: 286/428 - 8.94s\n",
      "Dataloader: 287/428 - 8.61s\n",
      "Dataloader: 288/428 - 9.07s\n",
      "Dataloader: 289/428 - 9.03s\n",
      "Dataloader: 290/428 - 8.95s\n",
      "Dataloader: 291/428 - 9.19s\n",
      "Dataloader: 292/428 - 9.22s\n",
      "Dataloader: 293/428 - 10.16s\n",
      "Dataloader: 294/428 - 8.78s\n",
      "Dataloader: 295/428 - 8.21s\n",
      "Dataloader: 296/428 - 8.68s\n",
      "Dataloader: 297/428 - 8.91s\n",
      "Dataloader: 298/428 - 9.54s\n",
      "Dataloader: 299/428 - 9.06s\n",
      "Dataloader: 300/428 - 9.01s\n",
      "Dataloader: 301/428 - 9.03s\n",
      "Dataloader: 302/428 - 8.58s\n",
      "Dataloader: 303/428 - 8.72s\n",
      "Dataloader: 304/428 - 10.88s\n",
      "Dataloader: 305/428 - 10.54s\n",
      "Dataloader: 306/428 - 9.08s\n",
      "Dataloader: 307/428 - 9.24s\n",
      "Dataloader: 308/428 - 8.75s\n",
      "Dataloader: 309/428 - 9.02s\n",
      "Dataloader: 310/428 - 8.53s\n",
      "Dataloader: 311/428 - 9.41s\n",
      "Dataloader: 312/428 - 8.85s\n",
      "Dataloader: 313/428 - 9.29s\n",
      "Dataloader: 314/428 - 9.24s\n",
      "Dataloader: 315/428 - 9.12s\n",
      "Dataloader: 316/428 - 8.31s\n",
      "Dataloader: 317/428 - 9.47s\n",
      "Dataloader: 318/428 - 11.15s\n",
      "Dataloader: 319/428 - 8.95s\n",
      "Dataloader: 320/428 - 8.84s\n",
      "Dataloader: 321/428 - 9.11s\n",
      "Dataloader: 322/428 - 8.88s\n",
      "Dataloader: 323/428 - 9.04s\n",
      "Dataloader: 324/428 - 9.18s\n",
      "Dataloader: 325/428 - 8.73s\n",
      "Dataloader: 326/428 - 9.53s\n",
      "Dataloader: 327/428 - 8.6s\n",
      "Dataloader: 328/428 - 9.57s\n",
      "Dataloader: 329/428 - 9.84s\n",
      "Dataloader: 330/428 - 8.1s\n",
      "Dataloader: 331/428 - 8.37s\n",
      "Dataloader: 332/428 - 9.09s\n",
      "Dataloader: 333/428 - 8.96s\n",
      "Dataloader: 334/428 - 8.52s\n",
      "Dataloader: 335/428 - 8.77s\n",
      "Dataloader: 336/428 - 8.53s\n",
      "Dataloader: 337/428 - 10.44s\n",
      "Dataloader: 338/428 - 9.08s\n",
      "Dataloader: 339/428 - 10.24s\n",
      "Dataloader: 340/428 - 9.36s\n",
      "Dataloader: 341/428 - 8.15s\n",
      "Dataloader: 342/428 - 8.55s\n",
      "Dataloader: 343/428 - 8.42s\n",
      "Dataloader: 344/428 - 9.08s\n",
      "Dataloader: 345/428 - 9.17s\n",
      "Dataloader: 346/428 - 8.98s\n",
      "Dataloader: 347/428 - 8.76s\n",
      "Dataloader: 348/428 - 8.6s\n",
      "Dataloader: 349/428 - 8.51s\n",
      "Dataloader: 350/428 - 8.87s\n",
      "Dataloader: 351/428 - 8.86s\n",
      "Dataloader: 352/428 - 8.75s\n",
      "Dataloader: 353/428 - 10.09s\n",
      "Dataloader: 354/428 - 9.03s\n",
      "Dataloader: 355/428 - 8.51s\n",
      "Dataloader: 356/428 - 8.99s\n",
      "Dataloader: 357/428 - 9.94s\n",
      "Dataloader: 358/428 - 8.85s\n",
      "Dataloader: 359/428 - 8.9s\n",
      "Dataloader: 360/428 - 9.1s\n",
      "Dataloader: 361/428 - 8.6s\n",
      "Dataloader: 362/428 - 9.13s\n",
      "Dataloader: 363/428 - 9.4s\n",
      "Dataloader: 364/428 - 8.89s\n",
      "Dataloader: 365/428 - 8.94s\n",
      "Dataloader: 366/428 - 9.26s\n",
      "Dataloader: 367/428 - 8.49s\n",
      "Dataloader: 368/428 - 8.42s\n",
      "Dataloader: 369/428 - 9.32s\n",
      "Dataloader: 370/428 - 8.92s\n",
      "Dataloader: 371/428 - 8.93s\n",
      "Dataloader: 372/428 - 9.14s\n",
      "Dataloader: 373/428 - 10.2s\n",
      "Dataloader: 374/428 - 9.81s\n",
      "Dataloader: 375/428 - 9.05s\n",
      "Dataloader: 376/428 - 8.92s\n",
      "Dataloader: 377/428 - 8.82s\n",
      "Dataloader: 378/428 - 9.37s\n",
      "Dataloader: 379/428 - 9.34s\n",
      "Dataloader: 380/428 - 8.42s\n",
      "Dataloader: 381/428 - 8.81s\n",
      "Dataloader: 382/428 - 9.94s\n",
      "Dataloader: 383/428 - 8.98s\n",
      "Dataloader: 384/428 - 8.52s\n",
      "Dataloader: 385/428 - 8.92s\n",
      "Dataloader: 386/428 - 9.65s\n",
      "Dataloader: 387/428 - 8.51s\n",
      "Dataloader: 388/428 - 8.6s\n",
      "Dataloader: 389/428 - 8.95s\n",
      "Dataloader: 390/428 - 9.24s\n",
      "Dataloader: 391/428 - 9.3s\n",
      "Dataloader: 392/428 - 9.89s\n",
      "Dataloader: 393/428 - 8.86s\n",
      "Dataloader: 394/428 - 8.88s\n",
      "Dataloader: 395/428 - 9.32s\n",
      "Dataloader: 396/428 - 8.55s\n",
      "Dataloader: 397/428 - 8.91s\n",
      "Dataloader: 398/428 - 9.73s\n",
      "Dataloader: 399/428 - 9.24s\n",
      "Dataloader: 400/428 - 8.48s\n",
      "Dataloader: 401/428 - 8.34s\n",
      "Dataloader: 402/428 - 8.83s\n",
      "Dataloader: 403/428 - 9.09s\n",
      "Dataloader: 404/428 - 9.12s\n",
      "Dataloader: 405/428 - 8.31s\n",
      "Dataloader: 406/428 - 10.0s\n",
      "Dataloader: 407/428 - 9.27s\n",
      "Dataloader: 408/428 - 8.45s\n",
      "Dataloader: 409/428 - 9.29s\n",
      "Dataloader: 410/428 - 8.81s\n",
      "Dataloader: 411/428 - 9.2s\n",
      "Dataloader: 412/428 - 9.22s\n",
      "Dataloader: 413/428 - 8.68s\n",
      "Dataloader: 414/428 - 9.17s\n",
      "Dataloader: 415/428 - 8.53s\n",
      "Dataloader: 416/428 - 8.82s\n",
      "Dataloader: 417/428 - 9.08s\n",
      "Dataloader: 418/428 - 8.62s\n",
      "Dataloader: 419/428 - 9.52s\n",
      "Dataloader: 420/428 - 9.48s\n",
      "Dataloader: 421/428 - 8.47s\n",
      "Dataloader: 422/428 - 8.69s\n",
      "Dataloader: 423/428 - 8.61s\n",
      "Dataloader: 424/428 - 9.51s\n",
      "Dataloader: 425/428 - 9.6s\n",
      "Dataloader: 426/428 - 9.09s\n",
      "Dataloader: 427/428 - 8.62s\n",
      "Dataloader: 428/428 - 9.08s\n",
      "Epoch 2, Loss: 1.5537246460112455\n",
      "Dataloader: 1/428 - 3.09s\n",
      "Dataloader: 2/428 - 8.57s\n",
      "Dataloader: 3/428 - 9.52s\n",
      "Dataloader: 4/428 - 8.7s\n",
      "Dataloader: 5/428 - 10.19s\n",
      "Dataloader: 6/428 - 8.94s\n",
      "Dataloader: 7/428 - 9.04s\n",
      "Dataloader: 8/428 - 10.44s\n",
      "Dataloader: 9/428 - 10.49s\n",
      "Dataloader: 10/428 - 8.55s\n",
      "Dataloader: 11/428 - 8.69s\n",
      "Dataloader: 12/428 - 9.22s\n",
      "Dataloader: 13/428 - 8.25s\n",
      "Dataloader: 14/428 - 8.19s\n",
      "Dataloader: 15/428 - 8.98s\n",
      "Dataloader: 16/428 - 8.73s\n",
      "Dataloader: 17/428 - 9.28s\n",
      "Dataloader: 18/428 - 10.61s\n",
      "Dataloader: 19/428 - 9.41s\n",
      "Dataloader: 20/428 - 8.83s\n",
      "Dataloader: 21/428 - 9.04s\n",
      "Dataloader: 22/428 - 9.27s\n",
      "Dataloader: 23/428 - 8.91s\n",
      "Dataloader: 24/428 - 8.94s\n",
      "Dataloader: 25/428 - 8.99s\n",
      "Dataloader: 26/428 - 9.22s\n",
      "Dataloader: 27/428 - 8.52s\n",
      "Dataloader: 28/428 - 9.36s\n",
      "Dataloader: 29/428 - 9.37s\n",
      "Dataloader: 30/428 - 10.11s\n",
      "Dataloader: 31/428 - 9.27s\n",
      "Dataloader: 32/428 - 9.29s\n",
      "Dataloader: 33/428 - 9.27s\n",
      "Dataloader: 34/428 - 11.19s\n",
      "Dataloader: 35/428 - 8.1s\n",
      "Dataloader: 36/428 - 8.23s\n",
      "Dataloader: 37/428 - 8.82s\n",
      "Dataloader: 38/428 - 9.48s\n",
      "Dataloader: 39/428 - 8.63s\n",
      "Dataloader: 40/428 - 9.75s\n",
      "Dataloader: 41/428 - 8.73s\n",
      "Dataloader: 42/428 - 10.15s\n",
      "Dataloader: 43/428 - 8.47s\n",
      "Dataloader: 44/428 - 8.1s\n",
      "Dataloader: 45/428 - 9.26s\n",
      "Dataloader: 46/428 - 8.12s\n",
      "Dataloader: 47/428 - 8.07s\n",
      "Dataloader: 48/428 - 8.37s\n",
      "Dataloader: 49/428 - 8.65s\n",
      "Dataloader: 50/428 - 8.17s\n",
      "Dataloader: 51/428 - 9.37s\n",
      "Dataloader: 52/428 - 9.89s\n",
      "Dataloader: 53/428 - 8.84s\n",
      "Dataloader: 54/428 - 8.02s\n",
      "Dataloader: 55/428 - 8.0s\n",
      "Dataloader: 56/428 - 8.27s\n",
      "Dataloader: 57/428 - 7.89s\n",
      "Dataloader: 58/428 - 8.14s\n",
      "Dataloader: 59/428 - 8.35s\n",
      "Dataloader: 60/428 - 8.33s\n",
      "Dataloader: 61/428 - 8.32s\n",
      "Dataloader: 62/428 - 9.0s\n",
      "Dataloader: 63/428 - 8.67s\n",
      "Dataloader: 64/428 - 8.7s\n",
      "Dataloader: 65/428 - 8.67s\n",
      "Dataloader: 66/428 - 8.41s\n",
      "Dataloader: 67/428 - 9.59s\n",
      "Dataloader: 68/428 - 8.23s\n",
      "Dataloader: 69/428 - 8.37s\n",
      "Dataloader: 70/428 - 8.23s\n",
      "Dataloader: 71/428 - 8.55s\n",
      "Dataloader: 72/428 - 8.96s\n",
      "Dataloader: 73/428 - 7.92s\n",
      "Dataloader: 74/428 - 7.91s\n",
      "Dataloader: 75/428 - 8.78s\n",
      "Dataloader: 76/428 - 8.02s\n",
      "Dataloader: 77/428 - 9.14s\n",
      "Dataloader: 78/428 - 8.12s\n",
      "Dataloader: 79/428 - 8.89s\n",
      "Dataloader: 80/428 - 9.44s\n",
      "Dataloader: 81/428 - 8.98s\n",
      "Dataloader: 82/428 - 8.07s\n",
      "Dataloader: 83/428 - 8.11s\n",
      "Dataloader: 84/428 - 9.64s\n",
      "Dataloader: 85/428 - 8.19s\n",
      "Dataloader: 86/428 - 8.3s\n",
      "Dataloader: 87/428 - 9.03s\n",
      "Dataloader: 88/428 - 8.28s\n",
      "Dataloader: 89/428 - 8.35s\n",
      "Dataloader: 90/428 - 8.91s\n",
      "Dataloader: 91/428 - 8.32s\n",
      "Dataloader: 92/428 - 9.01s\n",
      "Dataloader: 93/428 - 9.55s\n",
      "Dataloader: 94/428 - 8.64s\n",
      "Dataloader: 95/428 - 8.11s\n",
      "Dataloader: 96/428 - 9.77s\n",
      "Dataloader: 97/428 - 11.25s\n",
      "Dataloader: 98/428 - 8.67s\n",
      "Dataloader: 99/428 - 8.35s\n",
      "Dataloader: 100/428 - 9.04s\n",
      "Dataloader: 101/428 - 8.59s\n",
      "Dataloader: 102/428 - 8.03s\n",
      "Dataloader: 103/428 - 8.6s\n",
      "Dataloader: 104/428 - 8.27s\n",
      "Dataloader: 105/428 - 8.68s\n",
      "Dataloader: 106/428 - 9.6s\n",
      "Dataloader: 107/428 - 8.25s\n",
      "Dataloader: 108/428 - 8.19s\n",
      "Dataloader: 109/428 - 8.4s\n",
      "Dataloader: 110/428 - 8.54s\n",
      "Dataloader: 111/428 - 9.08s\n",
      "Dataloader: 112/428 - 9.6s\n",
      "Dataloader: 113/428 - 9.05s\n",
      "Dataloader: 114/428 - 8.92s\n",
      "Dataloader: 115/428 - 9.11s\n",
      "Dataloader: 116/428 - 8.14s\n",
      "Dataloader: 117/428 - 8.21s\n",
      "Dataloader: 118/428 - 7.9s\n",
      "Dataloader: 119/428 - 8.22s\n",
      "Dataloader: 120/428 - 8.62s\n",
      "Dataloader: 121/428 - 9.21s\n",
      "Dataloader: 122/428 - 8.12s\n",
      "Dataloader: 123/428 - 8.9s\n",
      "Dataloader: 124/428 - 8.69s\n",
      "Dataloader: 125/428 - 9.9s\n",
      "Dataloader: 126/428 - 8.22s\n",
      "Dataloader: 127/428 - 8.11s\n",
      "Dataloader: 128/428 - 8.85s\n",
      "Dataloader: 129/428 - 8.1s\n",
      "Dataloader: 130/428 - 8.12s\n",
      "Dataloader: 131/428 - 8.16s\n",
      "Dataloader: 132/428 - 9.14s\n",
      "Dataloader: 133/428 - 8.56s\n",
      "Dataloader: 134/428 - 9.37s\n",
      "Dataloader: 135/428 - 9.95s\n",
      "Dataloader: 136/428 - 8.66s\n",
      "Dataloader: 137/428 - 10.17s\n",
      "Dataloader: 138/428 - 10.05s\n",
      "Dataloader: 139/428 - 9.59s\n",
      "Dataloader: 140/428 - 10.52s\n",
      "Dataloader: 141/428 - 9.35s\n",
      "Dataloader: 142/428 - 8.77s\n",
      "Dataloader: 143/428 - 8.76s\n",
      "Dataloader: 144/428 - 8.56s\n",
      "Dataloader: 145/428 - 9.28s\n",
      "Dataloader: 146/428 - 8.47s\n",
      "Dataloader: 147/428 - 8.4s\n",
      "Dataloader: 148/428 - 8.95s\n",
      "Dataloader: 149/428 - 10.5s\n",
      "Dataloader: 150/428 - 9.32s\n",
      "Dataloader: 151/428 - 8.3s\n",
      "Dataloader: 152/428 - 9.33s\n",
      "Dataloader: 153/428 - 8.28s\n",
      "Dataloader: 154/428 - 9.12s\n",
      "Dataloader: 155/428 - 9.3s\n",
      "Dataloader: 156/428 - 8.91s\n",
      "Dataloader: 157/428 - 8.41s\n",
      "Dataloader: 158/428 - 8.51s\n",
      "Dataloader: 159/428 - 8.63s\n",
      "Dataloader: 160/428 - 8.85s\n",
      "Dataloader: 161/428 - 8.96s\n",
      "Dataloader: 162/428 - 8.49s\n",
      "Dataloader: 163/428 - 9.3s\n",
      "Dataloader: 164/428 - 8.28s\n",
      "Dataloader: 165/428 - 8.45s\n",
      "Dataloader: 166/428 - 8.42s\n",
      "Dataloader: 167/428 - 9.08s\n",
      "Dataloader: 168/428 - 9.22s\n",
      "Dataloader: 169/428 - 8.38s\n",
      "Dataloader: 170/428 - 8.37s\n",
      "Dataloader: 171/428 - 8.16s\n",
      "Dataloader: 172/428 - 8.64s\n",
      "Dataloader: 173/428 - 8.26s\n",
      "Dataloader: 174/428 - 9.11s\n",
      "Dataloader: 175/428 - 8.39s\n",
      "Dataloader: 176/428 - 8.16s\n",
      "Dataloader: 177/428 - 8.13s\n",
      "Dataloader: 178/428 - 8.44s\n",
      "Dataloader: 179/428 - 8.69s\n",
      "Dataloader: 180/428 - 8.82s\n",
      "Dataloader: 181/428 - 8.76s\n",
      "Dataloader: 182/428 - 8.39s\n",
      "Dataloader: 183/428 - 8.23s\n",
      "Dataloader: 184/428 - 9.02s\n",
      "Dataloader: 185/428 - 9.23s\n",
      "Dataloader: 186/428 - 8.98s\n",
      "Dataloader: 187/428 - 8.24s\n",
      "Dataloader: 188/428 - 8.34s\n",
      "Dataloader: 189/428 - 9.25s\n",
      "Dataloader: 190/428 - 9.67s\n",
      "Dataloader: 191/428 - 9.5s\n",
      "Dataloader: 192/428 - 8.87s\n",
      "Dataloader: 193/428 - 9.82s\n",
      "Dataloader: 194/428 - 8.3s\n",
      "Dataloader: 195/428 - 8.11s\n",
      "Dataloader: 196/428 - 8.14s\n",
      "Dataloader: 197/428 - 8.84s\n",
      "Dataloader: 198/428 - 8.67s\n",
      "Dataloader: 199/428 - 8.12s\n",
      "Dataloader: 200/428 - 8.54s\n",
      "Dataloader: 201/428 - 8.44s\n",
      "Dataloader: 202/428 - 8.29s\n",
      "Dataloader: 203/428 - 8.36s\n",
      "Dataloader: 204/428 - 8.5s\n",
      "Dataloader: 205/428 - 8.54s\n",
      "Dataloader: 206/428 - 8.15s\n",
      "Dataloader: 207/428 - 9.31s\n",
      "Dataloader: 208/428 - 8.19s\n",
      "Dataloader: 209/428 - 9.24s\n",
      "Dataloader: 210/428 - 11.29s\n",
      "Dataloader: 211/428 - 9.43s\n",
      "Dataloader: 212/428 - 8.65s\n",
      "Dataloader: 213/428 - 8.41s\n",
      "Dataloader: 214/428 - 9.09s\n",
      "Dataloader: 215/428 - 9.43s\n",
      "Dataloader: 216/428 - 8.64s\n",
      "Dataloader: 217/428 - 8.57s\n",
      "Dataloader: 218/428 - 8.37s\n",
      "Dataloader: 219/428 - 8.79s\n",
      "Dataloader: 220/428 - 8.23s\n",
      "Dataloader: 221/428 - 8.44s\n",
      "Dataloader: 222/428 - 8.18s\n",
      "Dataloader: 223/428 - 8.8s\n",
      "Dataloader: 224/428 - 8.85s\n",
      "Dataloader: 225/428 - 9.54s\n",
      "Dataloader: 226/428 - 8.78s\n",
      "Dataloader: 227/428 - 9.92s\n",
      "Dataloader: 228/428 - 9.36s\n",
      "Dataloader: 229/428 - 8.05s\n",
      "Dataloader: 230/428 - 8.73s\n",
      "Dataloader: 231/428 - 8.66s\n",
      "Dataloader: 232/428 - 8.41s\n",
      "Dataloader: 233/428 - 8.87s\n",
      "Dataloader: 234/428 - 8.95s\n",
      "Dataloader: 235/428 - 8.39s\n",
      "Dataloader: 236/428 - 9.12s\n",
      "Dataloader: 237/428 - 8.46s\n",
      "Dataloader: 238/428 - 9.92s\n",
      "Dataloader: 239/428 - 9.72s\n",
      "Dataloader: 240/428 - 9.33s\n",
      "Dataloader: 241/428 - 8.12s\n",
      "Dataloader: 242/428 - 8.17s\n",
      "Dataloader: 243/428 - 8.88s\n",
      "Dataloader: 244/428 - 8.38s\n",
      "Dataloader: 245/428 - 9.07s\n",
      "Dataloader: 246/428 - 8.28s\n",
      "Dataloader: 247/428 - 8.24s\n",
      "Dataloader: 248/428 - 8.97s\n",
      "Dataloader: 249/428 - 8.72s\n",
      "Dataloader: 250/428 - 8.93s\n",
      "Dataloader: 251/428 - 8.85s\n",
      "Dataloader: 252/428 - 8.78s\n",
      "Dataloader: 253/428 - 9.76s\n",
      "Dataloader: 254/428 - 8.58s\n",
      "Dataloader: 255/428 - 8.11s\n",
      "Dataloader: 256/428 - 8.14s\n",
      "Dataloader: 257/428 - 8.61s\n",
      "Dataloader: 258/428 - 9.17s\n",
      "Dataloader: 259/428 - 8.59s\n",
      "Dataloader: 260/428 - 8.5s\n",
      "Dataloader: 261/428 - 8.84s\n",
      "Dataloader: 262/428 - 8.34s\n",
      "Dataloader: 263/428 - 8.87s\n",
      "Dataloader: 264/428 - 8.96s\n",
      "Dataloader: 265/428 - 8.72s\n",
      "Dataloader: 266/428 - 8.41s\n",
      "Dataloader: 267/428 - 8.37s\n",
      "Dataloader: 268/428 - 8.31s\n",
      "Dataloader: 269/428 - 8.79s\n",
      "Dataloader: 270/428 - 8.38s\n",
      "Dataloader: 271/428 - 9.0s\n",
      "Dataloader: 272/428 - 8.56s\n",
      "Dataloader: 273/428 - 8.29s\n",
      "Dataloader: 274/428 - 9.02s\n",
      "Dataloader: 275/428 - 9.78s\n",
      "Dataloader: 276/428 - 8.29s\n",
      "Dataloader: 277/428 - 8.17s\n",
      "Dataloader: 278/428 - 8.33s\n",
      "Dataloader: 279/428 - 8.7s\n",
      "Dataloader: 280/428 - 8.73s\n",
      "Dataloader: 281/428 - 8.48s\n",
      "Dataloader: 282/428 - 9.37s\n",
      "Dataloader: 283/428 - 10.15s\n",
      "Dataloader: 284/428 - 9.62s\n",
      "Dataloader: 285/428 - 9.1s\n",
      "Dataloader: 286/428 - 8.53s\n",
      "Dataloader: 287/428 - 8.79s\n",
      "Dataloader: 288/428 - 9.05s\n",
      "Dataloader: 289/428 - 10.29s\n",
      "Dataloader: 290/428 - 9.41s\n",
      "Dataloader: 291/428 - 8.84s\n",
      "Dataloader: 292/428 - 8.35s\n",
      "Dataloader: 293/428 - 10.14s\n",
      "Dataloader: 294/428 - 9.19s\n",
      "Dataloader: 295/428 - 8.39s\n",
      "Dataloader: 296/428 - 8.4s\n",
      "Dataloader: 297/428 - 8.47s\n",
      "Dataloader: 298/428 - 9.22s\n",
      "Dataloader: 299/428 - 8.4s\n",
      "Dataloader: 300/428 - 8.52s\n",
      "Dataloader: 301/428 - 8.17s\n",
      "Dataloader: 302/428 - 10.41s\n",
      "Dataloader: 303/428 - 8.46s\n",
      "Dataloader: 304/428 - 8.49s\n",
      "Dataloader: 305/428 - 9.14s\n",
      "Dataloader: 306/428 - 8.68s\n",
      "Dataloader: 307/428 - 8.38s\n",
      "Dataloader: 308/428 - 8.61s\n",
      "Dataloader: 309/428 - 8.6s\n",
      "Dataloader: 310/428 - 8.62s\n",
      "Dataloader: 311/428 - 8.59s\n",
      "Dataloader: 312/428 - 8.89s\n",
      "Dataloader: 313/428 - 9.6s\n",
      "Dataloader: 314/428 - 9.29s\n",
      "Dataloader: 315/428 - 9.34s\n",
      "Dataloader: 316/428 - 9.37s\n",
      "Dataloader: 317/428 - 8.15s\n",
      "Dataloader: 318/428 - 8.82s\n",
      "Dataloader: 319/428 - 8.41s\n",
      "Dataloader: 320/428 - 9.23s\n",
      "Dataloader: 321/428 - 9.02s\n",
      "Dataloader: 322/428 - 9.08s\n",
      "Dataloader: 323/428 - 8.57s\n",
      "Dataloader: 324/428 - 8.89s\n",
      "Dataloader: 325/428 - 9.19s\n",
      "Dataloader: 326/428 - 8.45s\n",
      "Dataloader: 327/428 - 9.62s\n",
      "Dataloader: 328/428 - 8.36s\n",
      "Dataloader: 329/428 - 8.45s\n",
      "Dataloader: 330/428 - 8.51s\n",
      "Dataloader: 331/428 - 8.56s\n",
      "Dataloader: 332/428 - 8.29s\n",
      "Dataloader: 333/428 - 8.07s\n",
      "Dataloader: 334/428 - 8.3s\n",
      "Dataloader: 335/428 - 8.1s\n",
      "Dataloader: 336/428 - 8.69s\n",
      "Dataloader: 337/428 - 8.88s\n",
      "Dataloader: 338/428 - 8.28s\n",
      "Dataloader: 339/428 - 10.13s\n",
      "Dataloader: 340/428 - 8.41s\n",
      "Dataloader: 341/428 - 8.34s\n",
      "Dataloader: 342/428 - 8.25s\n",
      "Dataloader: 343/428 - 8.67s\n",
      "Dataloader: 344/428 - 9.8s\n",
      "Dataloader: 345/428 - 10.0s\n",
      "Dataloader: 346/428 - 8.3s\n",
      "Dataloader: 347/428 - 8.09s\n",
      "Dataloader: 348/428 - 8.06s\n",
      "Dataloader: 349/428 - 8.18s\n",
      "Dataloader: 350/428 - 9.28s\n",
      "Dataloader: 351/428 - 8.55s\n",
      "Dataloader: 352/428 - 10.73s\n",
      "Dataloader: 353/428 - 8.37s\n",
      "Dataloader: 354/428 - 8.42s\n",
      "Dataloader: 355/428 - 7.9s\n",
      "Dataloader: 356/428 - 8.05s\n",
      "Dataloader: 357/428 - 10.01s\n",
      "Dataloader: 358/428 - 8.27s\n",
      "Dataloader: 359/428 - 8.47s\n",
      "Dataloader: 360/428 - 8.68s\n",
      "Dataloader: 361/428 - 8.21s\n",
      "Dataloader: 362/428 - 9.68s\n",
      "Dataloader: 363/428 - 9.21s\n",
      "Dataloader: 364/428 - 8.44s\n",
      "Dataloader: 365/428 - 8.57s\n",
      "Dataloader: 366/428 - 9.28s\n",
      "Dataloader: 367/428 - 11.47s\n",
      "Dataloader: 368/428 - 9.24s\n",
      "Dataloader: 369/428 - 8.51s\n",
      "Dataloader: 370/428 - 8.45s\n",
      "Dataloader: 371/428 - 8.38s\n",
      "Dataloader: 372/428 - 8.18s\n",
      "Dataloader: 373/428 - 8.98s\n",
      "Dataloader: 374/428 - 8.2s\n",
      "Dataloader: 375/428 - 8.55s\n",
      "Dataloader: 376/428 - 8.73s\n",
      "Dataloader: 377/428 - 8.44s\n",
      "Dataloader: 378/428 - 8.67s\n",
      "Dataloader: 379/428 - 8.56s\n",
      "Dataloader: 380/428 - 9.34s\n",
      "Dataloader: 381/428 - 8.89s\n",
      "Dataloader: 382/428 - 8.8s\n",
      "Dataloader: 383/428 - 8.35s\n",
      "Dataloader: 384/428 - 9.55s\n",
      "Dataloader: 385/428 - 8.49s\n",
      "Dataloader: 386/428 - 10.31s\n",
      "Dataloader: 387/428 - 9.4s\n",
      "Dataloader: 388/428 - 9.14s\n",
      "Dataloader: 389/428 - 8.5s\n",
      "Dataloader: 390/428 - 8.49s\n",
      "Dataloader: 391/428 - 8.38s\n",
      "Dataloader: 392/428 - 8.28s\n",
      "Dataloader: 393/428 - 8.61s\n",
      "Dataloader: 394/428 - 8.85s\n",
      "Dataloader: 395/428 - 8.95s\n",
      "Dataloader: 396/428 - 9.14s\n",
      "Dataloader: 397/428 - 8.21s\n",
      "Dataloader: 398/428 - 8.75s\n",
      "Dataloader: 399/428 - 8.16s\n",
      "Dataloader: 400/428 - 8.97s\n",
      "Dataloader: 401/428 - 9.18s\n",
      "Dataloader: 402/428 - 10.21s\n",
      "Dataloader: 403/428 - 9.71s\n",
      "Dataloader: 404/428 - 8.57s\n",
      "Dataloader: 405/428 - 8.44s\n",
      "Dataloader: 406/428 - 8.61s\n",
      "Dataloader: 407/428 - 8.67s\n",
      "Dataloader: 408/428 - 8.45s\n",
      "Dataloader: 409/428 - 8.46s\n",
      "Dataloader: 410/428 - 8.61s\n",
      "Dataloader: 411/428 - 8.86s\n",
      "Dataloader: 412/428 - 8.07s\n",
      "Dataloader: 413/428 - 8.95s\n",
      "Dataloader: 414/428 - 8.97s\n",
      "Dataloader: 415/428 - 8.4s\n",
      "Dataloader: 416/428 - 8.55s\n",
      "Dataloader: 417/428 - 9.57s\n",
      "Dataloader: 418/428 - 8.91s\n",
      "Dataloader: 419/428 - 9.63s\n",
      "Dataloader: 420/428 - 8.57s\n",
      "Dataloader: 421/428 - 8.63s\n",
      "Dataloader: 422/428 - 8.21s\n",
      "Dataloader: 423/428 - 9.11s\n",
      "Dataloader: 424/428 - 8.94s\n",
      "Dataloader: 425/428 - 8.33s\n",
      "Dataloader: 426/428 - 8.13s\n",
      "Dataloader: 427/428 - 8.83s\n",
      "Dataloader: 428/428 - 8.17s\n",
      "Epoch 3, Loss: 1.5209998374230393\n"
     ]
    }
   ],
   "source": [
    "input_channels = 8\n",
    "num_classes = 6  \n",
    "\n",
    "\n",
    "model = CNNLSTM(in_channels=input_channels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "s = 0\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        print(f\"Dataloader: {i+1}/{len(train_dataloader)} - {round(time.time()-s,2)}s\")\n",
    "        s = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'epoch_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.2469233327379852\n",
      "Epoch 2, Loss: 1.22542110767877\n",
      "Epoch 3, Loss: 1.2178817504198751\n",
      "Epoch 4, Loss: 1.2123543588079024\n"
     ]
    }
   ],
   "source": [
    "#train model more\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        s = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader)}\")\n",
    "    torch.save(model.state_dict(), f'model_epoch_{num_epochs}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, _ \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[0;32m---> 61\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mappend(probabilities\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 37\u001b[0m, in \u001b[0;36mCNNLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x)\n\u001b[0;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test_path = 'test.csv'\n",
    "# TEST_EEG_PATH = 'test_eegs'\n",
    "\n",
    "# class TestEEGDataset(Dataset):\n",
    "#     def __init__(self, csv_file, eeg_path):\n",
    "#         self.csv = pd.read_csv(csv_file)  # Load the CSV file\n",
    "#         self.eeg_path = eeg_path\n",
    "#         self.sos = self.butter_bandpass_filter_init()  # Initialize the Butterworth filter parameters\n",
    "#         self.FEATS = ['Fp1','T3','C3','O1','Fp2','C4','T4','O2']\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.csv)\n",
    "    \n",
    "#     def butter_bandpass_filter_init(self):\n",
    "#         lowcut = 0.5  # Set the low-frequency cutoff for bandpass filtering\n",
    "#         highcut = 45.0  # Set the high-frequency cutoff for bandpass filtering\n",
    "#         fs = 200.0  # Sampling frequency\n",
    "#         order = 5  # Filter order\n",
    "\n",
    "#         nyq = 0.5 * fs\n",
    "#         low = lowcut / nyq\n",
    "#         high = highcut / nyq\n",
    "#         sos = butter(order, [low, high], analog=False, btype='band', output='sos')  # Create second-order sections for the Butterworth filter\n",
    "#         return sos\n",
    "\n",
    "#     def butter_bandpass_filter(self, data):\n",
    "#         y = sosfilt(self.sos, data)\n",
    "#         return y\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         eeg_id = self.csv.loc[idx, 'eeg_id']\n",
    "#         eeg_file_path = \"/Users/chloenguyen/Downloads/eeg_data/test_eegs/3911565283.parquet\"  # Build the EEG data file path\n",
    "#         eeg_data = pd.read_parquet(eeg_file_path)[self.FEATS].values  # Load EEG data from Parquet file\n",
    "\n",
    "#         eeg_data = self.butter_bandpass_filter(eeg_data)  # Apply filtering to the data\n",
    "#         eeg_data = torch.tensor(eeg_data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "         \n",
    "#         # Select 10,000 data points from the middle\n",
    "#         mid_index = eeg_data.shape[0] // 2\n",
    "#         start_index = mid_index - 5000  # Offset 5000 data points to the left from the middle\n",
    "#         end_index = mid_index + 5000  # Offset 5000 data points to the right from the middle\n",
    "#         eeg_data = eeg_data[start_index:end_index]\n",
    "#         # Swap dimensions\n",
    "#         eeg_data = torch.transpose(eeg_data, 0, 1)\n",
    "        \n",
    "#         return eeg_data\n",
    "\n",
    "\n",
    "# testdataset = TestEEGDataset(test_path, TEST_EEG_PATH)\n",
    "\n",
    "\n",
    "# test_dataloader = DataLoader(testdataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "      \n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predictions.append(probabilities.cpu().numpy())\n",
    "\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the model architecture\n",
    "model1 = CNNLSTM(in_channels=8, num_classes=6)\n",
    "\n",
    "# Load the model weights\n",
    "model1.load_state_dict(torch.load(\"epoch_427.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# No need to track gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_dataloader:\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Store predictions\n",
    "        predictions.extend(output.numpy())\n",
    "\n",
    "# Convert list to numpy array\n",
    "predictions = np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [  24   49   10    3   13 3319]\n"
     ]
    }
   ],
   "source": [
    "predictions_class = np.argmax(predictions, axis=1)\n",
    "#count how many different classes\n",
    "unique, counts = np.unique(predictions_class, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label indices: 5761     2\n",
      "5290     2\n",
      "14548    5\n",
      "6914     5\n",
      "1346     2\n",
      "        ..\n",
      "1923     3\n",
      "5361     1\n",
      "14853    4\n",
      "10707    1\n",
      "8195     4\n",
      "Name: target, Length: 3418, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#fit the model to the test data\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Create a list of class names\n",
    "class_names = ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']\n",
    "\n",
    "# Create a dictionary to map class names to indices\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Create a dictionary to map indices to class names\n",
    "idx_to_class = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Convert test labels to indices\n",
    "test_label_idx = test_label.map(class_to_idx)\n",
    "\n",
    "print(f\"Test label indices: {test_label_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41135166764189585\n",
      "Precision: 0.14797719479476032\n",
      "Recall: 0.16528551334048938\n",
      "F1 Score: 0.10522387525600106\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, and F1 score\n",
    "accuracy = accuracy_score(test_label_idx, predictions_class)\n",
    "precision = precision_score(test_label_idx, predictions_class, average='macro')\n",
    "recall = recall_score(test_label_idx, predictions_class, average='macro')\n",
    "f1 = f1_score(test_label_idx, predictions_class, average='macro')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "conf = confusion_matrix(test_label_idx, predictions_class)\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(conf, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
