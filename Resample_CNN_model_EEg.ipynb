{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>LPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eeg_id  seizure_vote  lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "0  568657           0.0  0.000000      0.25   0.000000   0.166667    0.583333   \n",
       "1  582999           0.0  0.857143      0.00   0.071429   0.000000    0.071429   \n",
       "2  642382           0.0  0.000000      0.00   0.000000   0.000000    1.000000   \n",
       "3  751790           0.0  0.000000      1.00   0.000000   0.000000    0.000000   \n",
       "4  778705           0.0  0.000000      0.00   0.000000   0.000000    1.000000   \n",
       "\n",
       "  target  \n",
       "0  Other  \n",
       "1    LPD  \n",
       "2  Other  \n",
       "3    GPD  \n",
       "4  Other  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the normalized data \n",
    "import pandas as pd\n",
    "normalized_data = pd.read_csv(\"normalized_data.csv\")\n",
    "normalized_data.head(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Assuming you have your data in X and y variables, where X is the feature matrix and y is the target variable\n",
    "X = normalized_data\n",
    "y = normalized_data['target']\n",
    "\n",
    "# Count the number of samples in each class\n",
    "class_counts = y.value_counts()\n",
    "\n",
    "# Find the minimum number of samples among all classes\n",
    "min_samples = class_counts.min()\n",
    "\n",
    "# Resample each class to have the same number of samples\n",
    "resampled_X = pd.DataFrame()\n",
    "resampled_y = pd.Series()\n",
    "for class_label in class_counts.index:\n",
    "    # Get the samples belonging to the current class\n",
    "    class_samples = X[y == class_label]\n",
    "    \n",
    "    # Resample the class to have the same number of samples as the minimum\n",
    "    resampled_class_samples = resample(class_samples, n_samples=min_samples, replace=False)\n",
    "    \n",
    "    # Append the resampled samples to the final resampled data\n",
    "    resampled_X = pd.concat([resampled_X, resampled_class_samples])\n",
    "    resampled_y = pd.concat([resampled_y, pd.Series([class_label] * min_samples)])\n",
    "\n",
    "# Now you have the resampled data in resampled_X and resampled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5496, 8), (5496,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_X.shape, resampled_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other      916\n",
       "Seizure    916\n",
       "LPD        916\n",
       "GRDA       916\n",
       "GPD        916\n",
       "LRDA       916\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_X.head(5)\n",
    "#count the number of samples in each class\n",
    "resampled_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewrite the resampled data to a csv file\n",
    "resampled_X.to_csv(\"resampled_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_PATH = 'train_eegs/'\n",
    "train_path = 'resampled_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = resampled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy.signal import butter, sosfilt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from EGGDataset import EEGDataset\n",
    "dataset = EEGDataset(train_path, EEG_PATH)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, test_dataset, train_label, test_label = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=8, num_classes=6):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(160000, 6)\n",
    "        self.bn3 = nn.BatchNorm1d(6)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # self.lstm1 = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        # self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # self.attention = nn.Sequential(\n",
    "        #     nn.Linear(128 * 2, 64),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(64, 1)\n",
    "        # )\n",
    "\n",
    "        # self.fc = nn.Linear(128 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.flatten1(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of class names\n",
    "class_names = ['Seizure', 'LPD', 'GDP', 'LRDA', 'GRDA', 'Other']\n",
    "\n",
    "# Create a dictionary to map class names to indices\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Create a dictionary to map indices to class names\n",
    "idx_to_class = {idx: class_name for idx, class_name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader: 1/138 - 1715890066.53s\n",
      "Dataloader: 2/138 - 1.1s\n",
      "Dataloader: 3/138 - 1.16s\n",
      "Dataloader: 4/138 - 1.16s\n",
      "Dataloader: 5/138 - 1.16s\n",
      "Dataloader: 6/138 - 1.14s\n",
      "Dataloader: 7/138 - 1.28s\n",
      "Dataloader: 8/138 - 1.18s\n",
      "Dataloader: 9/138 - 1.16s\n",
      "Dataloader: 10/138 - 1.22s\n",
      "Dataloader: 11/138 - 1.2s\n",
      "Dataloader: 12/138 - 1.14s\n",
      "Dataloader: 13/138 - 1.24s\n",
      "Dataloader: 14/138 - 1.3s\n",
      "Dataloader: 15/138 - 1.23s\n",
      "Dataloader: 16/138 - 1.21s\n",
      "Dataloader: 17/138 - 1.33s\n",
      "Dataloader: 18/138 - 1.19s\n",
      "Dataloader: 19/138 - 1.36s\n",
      "Dataloader: 20/138 - 1.18s\n",
      "Dataloader: 21/138 - 1.17s\n",
      "Dataloader: 22/138 - 1.17s\n",
      "Dataloader: 23/138 - 1.18s\n",
      "Dataloader: 24/138 - 1.15s\n",
      "Dataloader: 25/138 - 1.18s\n",
      "Dataloader: 26/138 - 1.2s\n",
      "Dataloader: 27/138 - 1.17s\n",
      "Dataloader: 28/138 - 1.17s\n",
      "Dataloader: 29/138 - 1.17s\n",
      "Dataloader: 30/138 - 1.14s\n",
      "Dataloader: 31/138 - 1.18s\n",
      "Dataloader: 32/138 - 1.16s\n",
      "Dataloader: 33/138 - 1.14s\n",
      "Dataloader: 34/138 - 1.17s\n",
      "Dataloader: 35/138 - 1.16s\n",
      "Dataloader: 36/138 - 1.14s\n",
      "Dataloader: 37/138 - 1.16s\n",
      "Dataloader: 38/138 - 1.16s\n",
      "Dataloader: 39/138 - 1.15s\n",
      "Dataloader: 40/138 - 1.16s\n",
      "Dataloader: 41/138 - 1.17s\n",
      "Dataloader: 42/138 - 1.14s\n",
      "Dataloader: 43/138 - 1.16s\n",
      "Dataloader: 44/138 - 1.16s\n",
      "Dataloader: 45/138 - 1.15s\n",
      "Dataloader: 46/138 - 1.16s\n",
      "Dataloader: 47/138 - 1.26s\n",
      "Dataloader: 48/138 - 1.13s\n",
      "Dataloader: 49/138 - 1.16s\n",
      "Dataloader: 50/138 - 1.16s\n",
      "Dataloader: 51/138 - 1.15s\n",
      "Dataloader: 52/138 - 1.18s\n",
      "Dataloader: 53/138 - 1.16s\n",
      "Dataloader: 54/138 - 1.16s\n",
      "Dataloader: 55/138 - 1.29s\n",
      "Dataloader: 56/138 - 1.16s\n",
      "Dataloader: 57/138 - 1.14s\n",
      "Dataloader: 58/138 - 1.18s\n",
      "Dataloader: 59/138 - 1.17s\n",
      "Dataloader: 60/138 - 1.14s\n",
      "Dataloader: 61/138 - 1.17s\n",
      "Dataloader: 62/138 - 1.17s\n",
      "Dataloader: 63/138 - 1.14s\n",
      "Dataloader: 64/138 - 1.16s\n",
      "Dataloader: 65/138 - 1.17s\n",
      "Dataloader: 66/138 - 1.16s\n",
      "Dataloader: 67/138 - 1.16s\n",
      "Dataloader: 68/138 - 1.17s\n",
      "Dataloader: 69/138 - 1.15s\n",
      "Dataloader: 70/138 - 1.17s\n",
      "Dataloader: 71/138 - 1.16s\n",
      "Dataloader: 72/138 - 1.15s\n",
      "Dataloader: 73/138 - 1.18s\n",
      "Dataloader: 74/138 - 1.15s\n",
      "Dataloader: 75/138 - 1.14s\n",
      "Dataloader: 76/138 - 1.16s\n",
      "Dataloader: 77/138 - 1.16s\n",
      "Dataloader: 78/138 - 1.15s\n",
      "Dataloader: 79/138 - 1.17s\n",
      "Dataloader: 80/138 - 1.14s\n",
      "Dataloader: 81/138 - 1.16s\n",
      "Dataloader: 82/138 - 1.16s\n",
      "Dataloader: 83/138 - 1.16s\n",
      "Dataloader: 84/138 - 1.15s\n",
      "Dataloader: 85/138 - 1.18s\n",
      "Dataloader: 86/138 - 1.17s\n",
      "Dataloader: 87/138 - 1.15s\n",
      "Dataloader: 88/138 - 1.16s\n",
      "Dataloader: 89/138 - 1.16s\n",
      "Dataloader: 90/138 - 1.16s\n",
      "Dataloader: 91/138 - 1.16s\n",
      "Dataloader: 92/138 - 1.17s\n",
      "Dataloader: 93/138 - 1.15s\n",
      "Dataloader: 94/138 - 1.16s\n",
      "Dataloader: 95/138 - 1.15s\n",
      "Dataloader: 96/138 - 1.14s\n",
      "Dataloader: 97/138 - 1.16s\n",
      "Dataloader: 98/138 - 1.26s\n",
      "Dataloader: 99/138 - 1.16s\n",
      "Dataloader: 100/138 - 1.16s\n",
      "Dataloader: 101/138 - 1.16s\n",
      "Dataloader: 102/138 - 1.14s\n",
      "Dataloader: 103/138 - 1.18s\n",
      "Dataloader: 104/138 - 1.16s\n",
      "Dataloader: 105/138 - 1.16s\n",
      "Dataloader: 106/138 - 1.17s\n",
      "Dataloader: 107/138 - 1.16s\n",
      "Dataloader: 108/138 - 1.15s\n",
      "Dataloader: 109/138 - 1.16s\n",
      "Dataloader: 110/138 - 1.16s\n",
      "Dataloader: 111/138 - 1.15s\n",
      "Dataloader: 112/138 - 1.16s\n",
      "Dataloader: 113/138 - 1.18s\n",
      "Dataloader: 114/138 - 1.15s\n",
      "Dataloader: 115/138 - 1.35s\n",
      "Dataloader: 116/138 - 1.3s\n",
      "Dataloader: 117/138 - 1.29s\n",
      "Dataloader: 118/138 - 1.2s\n",
      "Dataloader: 119/138 - 1.23s\n",
      "Dataloader: 120/138 - 1.2s\n",
      "Dataloader: 121/138 - 1.21s\n",
      "Dataloader: 122/138 - 1.17s\n",
      "Dataloader: 123/138 - 1.15s\n",
      "Dataloader: 124/138 - 1.26s\n",
      "Dataloader: 125/138 - 1.24s\n",
      "Dataloader: 126/138 - 1.19s\n",
      "Dataloader: 127/138 - 1.19s\n",
      "Dataloader: 128/138 - 1.19s\n",
      "Dataloader: 129/138 - 1.17s\n",
      "Dataloader: 130/138 - 1.17s\n",
      "Dataloader: 131/138 - 1.28s\n",
      "Dataloader: 132/138 - 1.2s\n",
      "Dataloader: 133/138 - 1.36s\n",
      "Dataloader: 134/138 - 1.34s\n",
      "Dataloader: 135/138 - 1.22s\n",
      "Dataloader: 136/138 - 1.27s\n",
      "Dataloader: 137/138 - 1.25s\n",
      "Dataloader: 138/138 - 1.3s\n",
      "Epoch 1, Loss: 1.7790661367817202\n"
     ]
    }
   ],
   "source": [
    "input_channels = 8\n",
    "num_classes = 6  \n",
    "\n",
    "\n",
    "model = CNNLSTM(in_channels=input_channels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "s = 0\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        print(f\"Dataloader: {i+1}/{len(train_dataloader)} - {round(time.time()-s,2)}s\")\n",
    "        s = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4258839617604795\n",
      "Epoch 2, Loss: 1.4231592788212541\n",
      "Epoch 3, Loss: 1.4227589515672214\n",
      "Epoch 4, Loss: 1.4155353083126787\n",
      "Epoch 5, Loss: 1.4155676045279573\n"
     ]
    }
   ],
   "source": [
    "#train model more\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        s = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader)}\")\n",
    "    torch.save(model.state_dict(), f'model_epoch_{num_epochs}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'cnn_model_35_epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNLSTM(in_channels=input_channels, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load('cnn_model_20_epoch.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# No need to track gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_dataloader:\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Store predictions\n",
    "        predictions.extend(output.numpy())\n",
    "\n",
    "# Convert list to numpy array\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [ 86 253  13 169 109 470]\n"
     ]
    }
   ],
   "source": [
    "predictions_class = np.argmax(predictions, axis=1)\n",
    "#count how many different classes\n",
    "unique, counts = np.unique(predictions_class, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label indices: 27     1\n",
      "691    5\n",
      "26     1\n",
      "185    0\n",
      "429    3\n",
      "      ..\n",
      "177    4\n",
      "85     0\n",
      "654    1\n",
      "649    2\n",
      "138    4\n",
      "Length: 1100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#fit the model to the test data\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Create a list of class names\n",
    "class_names = ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']\n",
    "\n",
    "# Create a dictionary to map class names to indices\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Create a dictionary to map indices to class names\n",
    "idx_to_class = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Convert test labels to indices\n",
    "test_label_idx = test_label.map(class_to_idx)\n",
    "\n",
    "print(f\"Test label indices: {test_label_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other      203\n",
       "LPD        183\n",
       "LRDA       183\n",
       "GPD        181\n",
       "Seizure    177\n",
       "GRDA       173\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17636363636363636\n",
      "Precision: 0.1789872619193212\n",
      "Recall: 0.1697718865728338\n",
      "F1 Score: 0.146084254436242\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, and F1 score of 20_epoch model\n",
    "accuracy = accuracy_score(test_label_idx, predictions_class)\n",
    "precision = precision_score(test_label_idx, predictions_class, average='macro')\n",
    "recall = recall_score(test_label_idx, predictions_class, average='macro')\n",
    "f1 = f1_score(test_label_idx, predictions_class, average='macro')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = CNNLSTM(in_channels=input_channels, num_classes=num_classes)\n",
    "model2.load_state_dict(torch.load('cnn_model_35_epoch.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model2.eval()\n",
    "\n",
    "# Initialize list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# No need to track gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_dataloader:\n",
    "        # Forward pass\n",
    "        output = model2(data)\n",
    "\n",
    "        # Store predictions\n",
    "        predictions.extend(output.numpy())\n",
    "\n",
    "# Convert list to numpy array\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [  8 188 136   2 343 423]\n"
     ]
    }
   ],
   "source": [
    "predictions_class = np.argmax(predictions, axis=1)\n",
    "#count how many different classes\n",
    "unique, counts = np.unique(predictions_class, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1590909090909091\n",
      "Precision: 0.1051069786239386\n",
      "Recall: 0.15520983950485587\n",
      "F1 Score: 0.12014654112914391\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, and F1 score of 20_epoch model\n",
    "accuracy = accuracy_score(test_label_idx, predictions_class)\n",
    "precision = precision_score(test_label_idx, predictions_class, average='macro')\n",
    "recall = recall_score(test_label_idx, predictions_class, average='macro')\n",
    "f1 = f1_score(test_label_idx, predictions_class, average='macro')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
